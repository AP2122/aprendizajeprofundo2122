{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1ddYRQ9oevB"
   },
   "source": [
    "# Detector de objetos\n",
    "> Usando Icevision y FastAI para crear un detector de objetos.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [practica]\n",
    "- image: images/chart-preview.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se muestra cómo crear un modelo de detección de objetos usando la arquitectura Faster R-CNN. Para crear nuestro modelo vamos a utilizar la librería [IceVision](https://airctic.com/) que es una librería para crear modelos de detección usando FastAI.\n",
    "\n",
    "En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrdL7e0EoevG"
   },
   "source": [
    "## Librerías\n",
    "\n",
    "Comenzamos descargando la librería IceVision. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -> Reiniciar Entorno de ejecución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V43yXELfhMPH"
   },
   "outputs": [],
   "source": [
    "!pip install icevision[all] -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJwCHTouoevI"
   },
   "source": [
    "A continuación, cargamos aquellas librerías que son necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q70xWQlAhWkk"
   },
   "outputs": [],
   "source": [
    "from icevision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KORNZQ6aoevJ"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Para esta práctica vamos a usar como ejemplo el [Fruit Images for Object Detection dataset](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection). Este dataset consta de 240 imágenes de entrenamiento y 60 de test con tres categorías: manzanas, plátanos y naranjas. Los siguientes comandos descargan y descomprimen dicho dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlDrFbeOi9aG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://www.dropbox.com/s/1dsfd5rrmg3riqj/fruits.zip?dl=1 -O fruits.zip\n",
    "!unzip fruits.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWDIq33hjR_B",
    "outputId": "0e265afc-230d-4d91-e319-3eeed1b2769a"
   },
   "source": [
    "Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYuhmDGjhrZE"
   },
   "outputs": [],
   "source": [
    "path=Path('fruits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drA06KJToevL"
   },
   "source": [
    "Como en la práctica anterior, podemos ver el contenido de este directorio usando el comando `ls()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-RRrBKSny4F"
   },
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzUyQEi9oevM"
   },
   "source": [
    "Si exploráis el directorio podréis ver que hay dos carpetas (llamadas train y test), y que cada una de ellas contiene dos carpetas, llamadas images y labels. La carpeta images contiene las imágenes del dataset, y la carpeta labels contiene las anotaciones en formato Pascal VOC. Para cada imagen, hay un fichero xml con el mismo nombre que contiene su extensión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTcQ9VXooevN"
   },
   "source": [
    "## Icevision\n",
    "\n",
    "El proceso para crear y evaluar un modelo de IceVision consta de los siguientes pasos:\n",
    "1. Crear un parser para leer las imágenes y las anotaciones.\n",
    "2. Construir objetos record a partir de los parser. \n",
    "3. Crear los datasets a partir de los records y los aumentos que queramos aplicar. \n",
    "4. Crear un dataloader a partir de los datasets. \n",
    "5. Definir un modelo. \n",
    "6. Entrenar el modelo. \n",
    "7. Guardar el modelo.\n",
    "8. Usar el modelo para inferencia\n",
    "\n",
    "Vamos a ver en detalle cada uno de estos pasos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBisxz6UoevN"
   },
   "source": [
    "### Parser\n",
    "\n",
    "IceVision proporciona una serie de parsers definidos por defecto para leer las anotaciones en distintos formatos entre ellos Pascal VOC y COCO. También es posible crear parsers propios. A pesar de que existe un parser para el formato de nuestra anotación, vamos a ver cómo crear un parser desde cero. \n",
    "\n",
    "El primer paso es crear una plantilla para nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaJWAPVOoevO"
   },
   "outputs": [],
   "source": [
    "template_record = ObjectDetectionRecord()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFGdfYw3oevO"
   },
   "source": [
    "A continuación IceVision proporciona el método `generate_template` que nos proporciona los métodos que debemos implementar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWIUjB6ooevP"
   },
   "outputs": [],
   "source": [
    "Parser.generate_template(template_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpJWBuyWoevP"
   },
   "source": [
    "A continuación vamos a implementar nuestra clase con cada uno de esos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSjGOI5PmanQ"
   },
   "outputs": [],
   "source": [
    "# Cargamos la siguiente librería que nos servirá para leer ficheros XML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class MyParser(Parser):\n",
    "    \"\"\"Definimos el constructor de nuestra clase que va a recibir cuatro parámetros:\n",
    "       - La plantilla definida previamente.\n",
    "       - El path al directorio donde se encuentran las imágenes.\n",
    "       - El path al directorio donde se encuentran las anotaciones.\n",
    "       - Un objeto class_map con las clases que tiene nuestro dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, template_record,path_img,path_anotaciones,class_map):\n",
    "        super().__init__(template_record=template_record)\n",
    "        self.path_img = path_img\n",
    "        self.path_anotaciones= path_anotaciones\n",
    "        self.class_map = class_map\n",
    "\n",
    "    \"\"\"El método iter escanea el directorio de anotaciones y nos devuelve el nombre \n",
    "    de cada fichero. Dicho nombre será utilizado por el resto de método\"\"\"    \n",
    "    def __iter__(self):\n",
    "        with os.scandir(self.path_anotaciones) as ficheros:\n",
    "            for fichero in ficheros:\n",
    "                yield fichero.name\n",
    "                \n",
    "    \"\"\"El método len nos indica el número de elementos de los que consta nuestro \n",
    "    dataset\"\"\"\n",
    "    def __len__(self):\n",
    "        return len(self.path_anotaciones)\n",
    "\n",
    "    \"\"\"A partir del nombre del fichero de anotación, record_id debe devolver el identificador\n",
    "    (o nombre) de la imagen asociada\"\"\"\n",
    "    def record_id(self, o) -> Hashable: #o --> nombre de la anotación\n",
    "        return o[:o.find('.')]\n",
    "\n",
    "    \"\"\"A continuación deberíamos definir el método parse_fields, pero vamos a definir una serie\n",
    "    de definiciones previas que nos serán útiles\"\"\"\n",
    "\n",
    "    \"\"\"El método prepare recibe el nombre de un fichero de anotación como parámetro y realiza\n",
    "    una serie de labores de preprocesamiento sobre dicho fichero de anotación. En este caso lo procesa\n",
    "    usando la funcionalidad de la librería para trabajar con xml\"\"\"\n",
    "    def prepare(self, o):\n",
    "        tree = ET.parse(str(self.path_anotaciones)+'/'+str(o))\n",
    "        self._root = tree.getroot()\n",
    "\n",
    "    \"\"\"El método filepath a partir del nombre del fichero de anotación devuelve el path de \n",
    "    la imagen asociada\"\"\"\n",
    "    def filepath(self, o) -> Union[str, Path]:\n",
    "        path=Path(f\"{o[:o.find('.')]}.jpg\")\n",
    "        return self.path_img / path\n",
    "\n",
    "    \"\"\"La función image_width_height devuelve el ancho y el alto de una imagen a partir del nombre\n",
    "    del fichero de anotación\"\"\"\n",
    "    def image_width_height(self, o) -> Tuple[int, int]:\n",
    "        return get_img_size(str(self.path_img)+'/'+f\"{o[:o.find('.')]}.jpg\")\n",
    "\n",
    "    \"\"\"La función labels recibe el nombre del fichero de anotación y debe devolver una lista \n",
    "    con los identificadores de las clases contenidas en dicho fichero.\"\"\"\n",
    "    def labels(self, o) -> List[int]:\n",
    "        labels = []\n",
    "        for object in self._root.iter(\"object\"):\n",
    "            label = object.find(\"name\").text\n",
    "            label_id = self.class_map.get_by_name(label)\n",
    "            labels.append(label)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    \"\"\"La función bboxes recibe el nombre del fichero de anotación y debe devolver una lista \n",
    "    de bboxes que son las anotaciones contenidas en dicho fichero. El formato de cada BBOX es\n",
    "    xmin, ymin, xmax, ymax.\"\"\"\n",
    "    def bboxes(self, o) -> List[BBox]:\n",
    "        def to_int(x):\n",
    "            return int(float(x))\n",
    "\n",
    "        bboxes = []\n",
    "        for object in self._root.iter(\"object\"):\n",
    "            xml_bbox = object.find(\"bndbox\")\n",
    "            xmin = to_int(xml_bbox.find(\"xmin\").text)\n",
    "            ymin = to_int(xml_bbox.find(\"ymin\").text)\n",
    "            xmax = to_int(xml_bbox.find(\"xmax\").text)\n",
    "            ymax = to_int(xml_bbox.find(\"ymax\").text)\n",
    "\n",
    "            bbox = BBox.from_xyxy(xmin, ymin, xmax, ymax)\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "    \"\"\"Definimos a continuación el método parse_fields para cada elemento de nuestro \n",
    "    dataset proporcionamos:\n",
    "       - El path a la imagen.\n",
    "       - El tamaño de la imagen.\n",
    "       - El mapa de clases.\n",
    "       - Los rectángulos que indican cada uno de los objetos de la imagen.\n",
    "       - Las etiquetas de cada uno de los objetos de la imagen.\"\"\"\n",
    "    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n",
    "        self.prepare(o)\n",
    "        if is_new:\n",
    "            record.set_filepath(self.filepath(o))\n",
    "            record.set_img_size(self.image_width_height(o))\n",
    "            record.detection.set_class_map(self.class_map)\n",
    "        record.detection.add_bboxes(self.bboxes(o))\n",
    "        record.detection.add_labels(self.labels(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzb54sb2oevQ"
   },
   "source": [
    "Una vez que hemos definido nuestra clase para parsear las anotaciones de nuestro dataset, vamos a construir los objetos correspondientes. \n",
    "\n",
    "Lo primero que tenemos que hacer es construir nuestro `class_map` que es un objeto de la clase `ClassMap` y que contiene las clases de objetos de nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNbkkl6soevR"
   },
   "outputs": [],
   "source": [
    "class_map = ClassMap(['apple','banana','orange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qnt3HJUoevR"
   },
   "source": [
    "A continuación definimos nuestros parsers. Uno para leer el conjunto de entrenamiento, y otro para el de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnmVc1hMjjdQ"
   },
   "outputs": [],
   "source": [
    "trainPath = Path('fruits')/'train'\n",
    "parserTrain = AnotacionParser(trainPath/'images', trainPath/'labels', class_map)\n",
    "\n",
    "testPath = Path('fruits')/'test'\n",
    "parserTest = AnotacionParser(testPath/'images', testPath/'labels', class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOrLeTTyoevS"
   },
   "source": [
    "### Records\n",
    "\n",
    "Un record es un diccionario que contiene todos los campos parseados definidos en el proceso anterior. Mientras que cada parser es específico para cada anotación concreta, los objetos record tienen una estructura común. Para construir los records a partir de nuestros objetos `parser` debemos llamar al método `parse` e indicarle cómo se van a repartir los datos que se lean. \n",
    "\n",
    "Como siempre, vamos a dividir nuestro dataset en tres partes: un conjunto de entrenamiento, uno de validación y uno de test. Por lo tanto tendremos que construir tres records llamados `train_records`, `valid_records` y `test_records`. Los records de entrenamiento y validación los construiremos a partir de los datos de entrenamiento usando una partición 90/10. Mientras que el record de test se construye a partir del conjunto de test usándolo completamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyrgs5gJjp1t"
   },
   "outputs": [],
   "source": [
    "train_records, valid_records = parserTrain.parse(RandomSplitter((0.9, 0.1)))\n",
    "test_records,_= parserTest.parse(RandomSplitter((1.0, 0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmdJgckJoevS"
   },
   "source": [
    "### Transforms\n",
    "\n",
    "Las transformaciones o aumentos son una parte fundamental cuando estamos construyendo modelos en visión por computador. IceVision incluye por defecto la librería [Albumentations](https://github.com/albumentations-team/albumentations) que nos proporciona una gran cantidad de transformaciones. Además es capaz de gestionar los cambios en la anotación que son necesarios cuando se trabaja en detección de objetos. \n",
    "\n",
    "IceVision proporciona una función muy útil que es [`tfms.A.aug_tfms`](https://airctic.com/albumentations_tfms/) con una gran cantidad de transformaciones. Además podemos añadirle cualquier otra transformación de Albumentations. \n",
    "\n",
    "Para nuestro ejemplo vamos a usar las transformaciones sugeridas por defecto por IceVision y aplicar la técnica de presizing vista para clasificación de imágenes; además será necesario normalizar las imágenes al rango de las imágenes de ImageNet. Notar que las transformaciones se aplicarán solo al conjunto de entrenamiento. Para los conjuntos de validación y test únicamente tendremos que escalar la imagen al tamaño adecuado y normalizarla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gee0nYgk3F3"
   },
   "outputs": [],
   "source": [
    "presize = 512\n",
    "size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwUimo0Fk-KH"
   },
   "outputs": [],
   "source": [
    "train_tfms = tfms.A.Adapter(\n",
    "    [*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]\n",
    ")\n",
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9dP-WGRoevT"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "La clase `Dataset` sirve para combinar los records y las transformaciones. Debemos crear un dataset para nuestro conjunto de entrenamiento, otro para el conjunto de validación y otro para el de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoH2Nfgtk_-E"
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "test_ds = Dataset(test_records, valid_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXOSzVsNoevT"
   },
   "source": [
    "Una vez creados dichos datasets podemos mostrar imágenes de los mismos. En concreto la siguiente instrucción muestra imágenes del conjunto de entrenamiento a las cuáles se les han aplicado una serie de transformaciones. Es conveniente ejecutar esta visualización para comprobar que todo está correcto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uHT4HhElGgK"
   },
   "outputs": [],
   "source": [
    "samples = [train_ds[0] for _ in range(3)]\n",
    "show_samples(samples, ncols=3, class_map=class_map, denormalize_fn=denormalize_imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq2ZLsc8oevT"
   },
   "source": [
    "### DataLoaders\n",
    "\n",
    "Al igual que vimos para los modelos de clasificación de FastAI, el último paso es crear nuestros DataLoaders a partir de los datasets construidos anteriormente. Notar que cada modelo tiene su propio DataLoader. En este caso como vamos a crear un modelo de Faster RCNN debemos usar las siguientes instrucciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIplhRualIlE"
   },
   "outputs": [],
   "source": [
    "train_dl = models.torchvision.faster_rcnn.train_dl(train_ds, batch_size=8, num_workers=0, shuffle=True)\n",
    "valid_dl = models.torchvision.faster_rcnn.valid_dl(valid_ds, batch_size=8, num_workers=0, shuffle=False)\n",
    "test_dl = models.torchvision.faster_rcnn.valid_dl(test_ds, batch_size=8, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaNjaaByoevU"
   },
   "source": [
    "### Entrenando el modelo\n",
    "\n",
    "Para crear y entrenar nuestro modelo debemos crear un objeto `Learner` de FastAI. Para crear dicho objeto, lo primero que debemos hacer es construir un modelo con la arquitectura que queremos usar, en este caso Faster RCNN y con un backbone que es Resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGONOogQoevU"
   },
   "outputs": [],
   "source": [
    "model = models.torchvision.faster_rcnn.model(backbone=models.torchvision.faster_rcnn.backbones.resnet18_fpn(pretrained=True),\n",
    "                                       num_classes=len(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNfA-EvooevU"
   },
   "source": [
    "A continuación debemos proporcionar las métricas que queremos utilizar para evaluar el modelo. Por el momento la única métrica soportada por IceVision es el mAP de COCO, por lo tanto utilizaremos dicha métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxSm9QU4lO_K"
   },
   "outputs": [],
   "source": [
    "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoY_IcvsoevU"
   },
   "source": [
    "Ya estamos listos para construir nuestro `Learner`. Notar que dicho objeto se construye de manera distinta dependiendo de la arquitectura que queramos utilizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtuQ9Pj-lSoV"
   },
   "outputs": [],
   "source": [
    "learn = models.torchvision.faster_rcnn.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVc01_dDQntN"
   },
   "source": [
    "Ahora podemos entrenar nuestro modelo utilizando la técnica de fine tuning que vimos en clase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVZJNDqJlZi5"
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(10,freeze_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7SXFpUQoevV"
   },
   "source": [
    "Una vez finalizado el entrenamiento podemos guardar nuestro modelo del siguiente modo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYO1kWUDrngs"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'fasterRCNNFruits.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjNMmfKuoevV"
   },
   "source": [
    "### Evaluando el modelo\n",
    "\n",
    "Al igual que vimos para los modelos de clasificación, la métrica mostrada durante el proceso de entrenamiento se refiere al conjunto de validación, mientras que nos interesa saber el resultado obtenido para el conjunto de test. \n",
    "\n",
    "Para ello, lo primero que debemos hacer es construir un nuevo dataloader del siguiente modo, indicando que el conjunto de validación es el de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DrHxoeNRGAt"
   },
   "outputs": [],
   "source": [
    "newdl = fastai.DataLoaders(models.torchvision.faster_rcnn.fastai.convert_dataloader_to_fastai(train_dl),\n",
    "                           models.torchvision.faster_rcnn.fastai.convert_dataloader_to_fastai(test_dl)).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHrSYhKJoevV"
   },
   "source": [
    "A continuación modificamos el dataloader del objeto `Learn` que hemos entrenado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1hYzealQYYC"
   },
   "outputs": [],
   "source": [
    "learn.dls = newdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBvc7EyloevW"
   },
   "source": [
    "Por último evaluamos nuestro modelo usando el método `validate()`. Al igual que en el caso de los modelos de clasificación el método `validate()` devuelve dos valores, el valor de la pérdida y el valor de la métrica asociada al conjunto de validación, que en este caso es el de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms4LXdZ0LUV_"
   },
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1LLUhNSoevW"
   },
   "source": [
    "### Inferencia\n",
    "\n",
    "Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo. Para ello debemos crear un modelo con la arquitectura que utilizamos (Faster RCNN en nuestro caso), y posteriormente cargar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eHIEmkfPgZq"
   },
   "outputs": [],
   "source": [
    "model = models.torchvision.faster_rcnn.model(backbone=models.torchvision.faster_rcnn.backbones.resnet18_fpn,\n",
    "                                             num_classes=len(class_map))\n",
    "state_dict = torch.load('fasterRCNNFruits.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-5VatiXoevW"
   },
   "source": [
    "El siguiente paso es cargar la imagen, para lo que usaremos la librería `PIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq044_u5MHHP"
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TunuHhauMH_e"
   },
   "outputs": [],
   "source": [
    "img = PIL.Image.open('fruits/test/images/mixed_25.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oARekjsoevX"
   },
   "source": [
    "La siguiente instrucción permite mostrar la imagen que acabamos de cargar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGrZkmcDMJXI"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vawWUfmxoevX"
   },
   "source": [
    "Ya estaríamos listos para relizar las predicciones sobre la imagen. Sin embargo, cabe recordar que primero debemos reescalar las imágenes y normalizarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g-twflKMFo9"
   },
   "outputs": [],
   "source": [
    "infer_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size),tfms.A.Normalize()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfHLRtj4oevX"
   },
   "source": [
    "Ya podemos realizar las predicciones mediante el método ``end2end_detect``. Este método, que depende de la arquitectura que hayamos utilizado, recibe como parámetros la imagen sobre la que queremos realizar las predicciones, las transformaciones a aplicar, el modelo (movido a la CPU), el mapa de clases, y el nivel de confianza mínimo para realizar la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWMTK4xoMKnG"
   },
   "outputs": [],
   "source": [
    "pred_dict  = models.torchvision.faster_rcnn.end2end_detect(img, infer_tfms, model.to(\"cpu\"), class_map=class_map, detection_threshold=0.5)\n",
    "pred_dict['img']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practica2Instrucciones.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
