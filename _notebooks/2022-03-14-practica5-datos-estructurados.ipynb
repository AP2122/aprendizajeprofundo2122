{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjCK-mNI31hb"
   },
   "source": [
    "# Práctica 5: usando embeddings con datos estructurados\n",
    "> FastAI con datos estructurados\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [practica]\n",
    "- image: images/chart-preview.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se muestra cómo crear un modelo de clasificación para un dataset estructurado que contiene características categóricas usando una red neuronal. Para ello usaremos la librería [FastAI](https://www.fast.ai/).\n",
    "\n",
    "Aunque para esta práctica no es necesario el uso de GPU, las pruebas serán más rápidas si tienes esta opción activada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqrUno5P31hh"
   },
   "source": [
    "## Librerías\n",
    "\n",
    "Comenzamos actualizando la librería FastAI. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -> Reiniciar Entorno de ejecución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V43yXELfhMPH"
   },
   "outputs": [],
   "source": [
    "!pip install fastai -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HFLwugx31hk"
   },
   "source": [
    "Cargamos a continuación las librerías que necesitaremos en esta práctica que son la parte de datos estructurados de la librería fastAI y la librería pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dbyahqc31hl"
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOuB9EuYzAXH"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Para este ejemplo vamos a usar el [Adult Data Set](https://archive.ics.uci.edu/ml/datasets/Adult). Con este dataset se pretende ser capaz de predecir si una persona cobra más de 50000 dolares al año a partir de las siguientes características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xerdc7RCzAXH"
   },
   "outputs": [],
   "source": [
    "columns = ['age','workclass','fnlwgt','education',     \n",
    "          'education-num','marital-status',\n",
    "          'occupation', 'relationship', 'race',\n",
    "          'sex','capital-gain','capital-loss',\n",
    "          'hours-per-week','native-country','income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeYWE_QZzAXI"
   },
   "source": [
    "Vamos a descargar el dataset usando los siguientes comandos. Debemos descargar tanto el conjunto de entrenamiento como el de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Flfd-xkzAXI"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data -O train.csv\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test -O test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WYiy0qNzAXJ"
   },
   "source": [
    "Si inspeccionais el fichero de test veremos que la primera línea no proporciona información útil, y además los valores de la columna del salario incluyen un punto. Los siguientes comandos se encargan de eliminar la primera línea del fichero y de eliminar los puntos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guCp-dVSzAXJ"
   },
   "outputs": [],
   "source": [
    "# Eliminamos la primera línea del fichero de test\n",
    "!sed '1d' test.csv > tmpfile; mv tmpfile test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAlxjWOXzAXJ"
   },
   "outputs": [],
   "source": [
    "# Eliminamos los puntos del fichero de test\n",
    "!sed 's/\\.//' test.csv > tmpfile; mv tmpfile test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPvyMqlYzAXK"
   },
   "source": [
    "## Carga de datos\n",
    "\n",
    "Al igual que se trabajaba en la asignatura de aprendizaje automático pasamos a cargar ambos datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyOCMLnCzAXK"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',header=None,names=columns)\n",
    "df_test = pd.read_csv('test.csv',header=None,names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V1np19bzAXK"
   },
   "source": [
    "Para crear un modelo con datos estructurados en FastAI debemos:\n",
    "- Indicar la columna que queremos predecir, es decir la variable dependiente.\n",
    "- Las transformaciones que queremos aplicar a nuestro dataset. \n",
    "- Las características que son categóricas y continuas.\n",
    "\n",
    "Nuestra variable dependiente va a ser el salario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giHGL_JrzAXL"
   },
   "outputs": [],
   "source": [
    "dep_var = 'income'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzDG1xhzzAXL"
   },
   "source": [
    "Las transformaciones que vamos a aplicar nuestro dataset son:\n",
    "- Categorizar las variables categoricas de manera que se use un embedding.\n",
    "- Reemplazar aquellos valores que faltan en el dataset por la media del resto de valores de ese descriptor.\n",
    "- Normalizar los datos. \n",
    "\n",
    "Esto se consigue definiendo la siguiente lista de transformaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwY7WYmrzAXL"
   },
   "outputs": [],
   "source": [
    "procs = [Categorify, FillMissing,Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8aaPf7LzAXL"
   },
   "source": [
    "Por último debemos indicar qué variables son continuas (números) o categóricas. De esto se encarga automáticamente la función `cont_cat_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-hyi3o2zAXM"
   },
   "outputs": [],
   "source": [
    "cont,cat = cont_cat_split(df_train, 1, dep_var=dep_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yvMjWCPzAXM"
   },
   "source": [
    "Podemos ver a hora que realmente las variables continuas y categóricas han sido separadas de forma correcta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8fOfDEBzAXM"
   },
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imbQrb5gzAXM"
   },
   "outputs": [],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOulGTUHzAXM"
   },
   "source": [
    "## Entrenando un modelo\n",
    "\n",
    "El proceso para entrenar un modelo es el mismo que hemos visto para los modelos de clasificación de imágenes y podemos usar toda la funcionalidad vista hasta ahora. Comenzamos definiendo un dataloader usando el método estático `from_df` de la clase `TabularDataLoaders`. Como siempre debemos definir un conjunto de validación, para ello debemos indicar los índices del dataset que tomaremos como conjunto de validación. En este caso vamos a tomar un 20% de los índices de manera aleatoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmh2UNt5zAXN"
   },
   "outputs": [],
   "source": [
    "dlsTrain = TabularDataLoaders.from_df(df_train,'.',procs=procs,\n",
    "                                cat_names=cat,cont_names=cont,y_names='income',\n",
    "                                valid_idx=random.sample(range(0,len(df_train)),int(len(df_train)*0.2)),bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grPO2IBvzAXN"
   },
   "source": [
    "Ahora definimos nuestro learner que va a ser una red neuronal, a la cual le tenemos que indicar el número de capas y neuronas con las que queremos trabajar. En este caso usaremos una red neuronal con dos capas ocultas la primera con 500 neuronas y la segunda con 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44oVaPP3zAXN"
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(dlsTrain, layers=[500,250],metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZfKC6LwzAXN"
   },
   "source": [
    "Podemos ver la estructura del modelo que acabamos de construir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCwIBWfqzAXN"
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39GfwHAqzAXN"
   },
   "source": [
    "Podemos ver que tenemos un `TabularModel` con una serie de embeddings. La siguiente celda nos sirve para ver qué es lo que hace un `TabularModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXq0D6lgzAXN"
   },
   "outputs": [],
   "source": [
    "??TabularModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGvwUtayzAXO"
   },
   "source": [
    "Como hemos dicho tenemos toda la batería de funciones que ya conocemos, así que vamos a buscar el learning rate más adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzY6LUxVzAXO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aoq5CHw8zAXO"
   },
   "source": [
    "Pasamos ahora a entrenar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EWaocy1zAXO"
   },
   "outputs": [],
   "source": [
    "learn.fit(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRGrtAAuzAXO"
   },
   "source": [
    "## Evaluación\n",
    "\n",
    "Hemos logrado una accuracy del 84.50 en el conjunto de validación, pero nos interesa ver el rendimiento de nuestro modelo en el conjunto de test. Para ello debemos primero crear un dataloader con el conjunto de test en el que el conjunto de validación está formado por todos los elementos del conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dqSink9zAXO"
   },
   "outputs": [],
   "source": [
    "dlsTest = TabularDataLoaders.from_df(df_test,'.',procs=procs,\n",
    "                                cat_names=cat,cont_names=cont,y_names='income',\n",
    "                                valid_idx=list(range(0,len(df_test)-1)),bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0_GfzI4zAXP"
   },
   "source": [
    "Cambiamos ahora el dataloader del learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYGKTikozAXP"
   },
   "outputs": [],
   "source": [
    "learn.dls=dlsTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb3zlUHuzAXP"
   },
   "source": [
    "Y procedemos a obtener la accuracy de nuestro modelo en el conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUuj-bktzAXP"
   },
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4txSKdw6zAXP"
   },
   "source": [
    "También podemos usar nuestro modelo para hacer predicciones concretas con un ejemplo usando la función `predict` esto nos devolverá tres elementos: los datos sobre los que se ha realizado la predicción, la clase, y las probabilidades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnF4TjlFzAXP"
   },
   "outputs": [],
   "source": [
    "learn.predict(df_test.iloc[0][:-1])"
   ]
  }
 ],
}
