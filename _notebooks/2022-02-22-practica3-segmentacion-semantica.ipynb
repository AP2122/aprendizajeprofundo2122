{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjCK-mNI31hb"
   },
   "source": [
    "# Segmentación semántica\n",
    "> Usando FastAI para crear un modelo de segmentación semántica.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [practica]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se muestra cómo crear un modelo de segmentación semántica usando la arquitectura U-net incluida en la librería FastAI.\n",
    "\n",
    "En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqrUno5P31hh"
   },
   "source": [
    "## Librerías\n",
    "\n",
    "Comenzamos actualizando la librería FastAI. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -> Reiniciar Entorno de ejecución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V43yXELfhMPH"
   },
   "outputs": [],
   "source": [
    "!pip install fastai -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HFLwugx31hk"
   },
   "source": [
    "Cargamos a continuación las librerías que necesitaremos en esta práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dbyahqc31hl"
   },
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback import *\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzT62Ihz31hl"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Para esta práctica vamos a usar como dataset el proporcionado en el trabajo [Deep neural networks for grape bunch segmentation in natural images from a consumer‑grade camera](https://link.springer.com/article/10.1007/s11119-020-09736-0). Este dataset dedicado a la segmentación de racimos de uva consta de 66 imágenes de entrenamiento y 14 de test con 5 categorías: background, leaves, wood, pole, y grape. Los siguientes comandos descargan y descomprimen dicho dataset. En este notebook vamos a usar solo dos clases: background y grape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlDrFbeOi9aG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://www.dropbox.com/s/uknzc914w311web/dataset.zip?dl=1 -O dataset.zip\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWDIq33hjR_B",
    "outputId": "0e265afc-230d-4d91-e319-3eeed1b2769a"
   },
   "source": [
    "Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYuhmDGjhrZE"
   },
   "outputs": [],
   "source": [
    "path=Path('dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-CI0cPc31ho"
   },
   "source": [
    "Como en la práctica anterior, podemos ver el contenido de este directorio usando el comando `ls()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-RRrBKSny4F"
   },
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTpQDgD631hq"
   },
   "source": [
    "Si exploráis el directorio podréis ver que hay dos carpetas llamadas Images y Labels. La carpeta Images contiene las imágenes del dataset, y la carpeta Labels contiene las en forma de máscara. Para cada imagen, hay un fichero de anotación siguiendo la siguiente nomenclatura: si la imagen se llama color_xxx.jpg, su fichero de anotación es gt_xxx.png. El dataset está partido en entrenamiento y test como puede verse en las carpetas Images y Labels. Además, se proporcionan dos ficheros txt que van a contener las clases de los objetos que utilizaremos en esta práctica. El fichero codes.txt contiene solo dos clases (background y grape), mientras que el fichero codesAll.txt contiene todas las posibles clases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PsBLp-b31hq"
   },
   "outputs": [],
   "source": [
    "(path/'Images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1bVzeTl31hr"
   },
   "outputs": [],
   "source": [
    "(path/'Images/train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkvsBbHp31hr"
   },
   "outputs": [],
   "source": [
    "(path/'Labels/train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC2Vm-ez31hr"
   },
   "source": [
    "## Definiciones previas\n",
    "\n",
    "El proceso para entrenar nuestro modelo va a ser similar al visto en la práctica 1 para crear un modelo de clasificación. Sin embargo, para cargar nuestro dataset será necesario dar unas definiciones previas. Estas definiciones son necesarias para ajustar la carga del datos a la estructura de nuestro dataset. \n",
    "\n",
    "En primer lugar vamos a definir los paths donde se van a encontrar nuestras imágenes y sus etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEiz7ixO31hs"
   },
   "outputs": [],
   "source": [
    "path_images = path/\"Images\"\n",
    "path_labels = path/\"Labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbj7IgQC31hs"
   },
   "source": [
    "A continuación definimos el nombre que va a tener nuestra carpeta de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No24h55L31hs"
   },
   "outputs": [],
   "source": [
    "test_name = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf2a90o531ht"
   },
   "source": [
    "Seguidamente definimos una función que dado el path de una imagen nos devuelve el path de su anotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XE2yMD231ht"
   },
   "outputs": [],
   "source": [
    "def get_y_fn (x):\n",
    "    return Path(str(x).replace(\"Images\",\"Labels\").replace(\"color\",\"gt\").replace(\".jpg\",\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PuwCQsj31ht"
   },
   "source": [
    "Seguidamente cargamos las clases que pueden tener los píxeles de nuestra imágenes y lo almacenamos en una lista `codes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJ6FNPSw31ht"
   },
   "outputs": [],
   "source": [
    "codes = np.loadtxt(path/'codes.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPFtV3xq31hu"
   },
   "outputs": [],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo9eSTPV31hu"
   },
   "source": [
    "Podemos ahora ver alguna de las imágenes de nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKAqWyDt31hu"
   },
   "outputs": [],
   "source": [
    "img_f = path_images/'train/color_206.jpg'\n",
    "img = PILImage.create(img_f)\n",
    "img.show(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEKZQpD531hv"
   },
   "source": [
    "Y también la anotación asociada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7JbQcDF31hv"
   },
   "outputs": [],
   "source": [
    "mask = PILMask.create(get_y_fn(img_f))\n",
    "mask.show(figsize=(5, 5), alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9nu5-V931hw"
   },
   "source": [
    "Como podemos ver en la imagen anterior tenemos una máscara donde cada tipo de objeto de nuestra imagen tiene un color distinto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbBhTeWs31hw"
   },
   "source": [
    "## Partición del dataset\n",
    "\n",
    "Como en cualquier problema de machine learning debemos partir nuestro dataset en entrenamiento y test. En nuestro caso los datos ya están separados por lo que vamos a definir una función que nos permite diferenciarlos gracias a la estructura de carpetas que usamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G7g5gX031hw"
   },
   "outputs": [],
   "source": [
    "def ParentSplitter(x):\n",
    "    return Path(x).parent.name==test_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cDC3Rvw31hw"
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "Al igual que con los modelos definidos en prácticas anteriores podemos usar técnicas de aumento de datos, para lo que usaremos la librería Albumentations. Recordar que dichas transformaciones no deben aplicarse solo a la imagen sino también a su anotación. Para ello vamos a definir una clase que hereda de la clase `ItemTransform` y que nos va a permitir realizar transformaciones sobre pares (imagen,máscara). \n",
    "\n",
    "La clase `ItemTransform` tiene un método `encodes` que es el encargado de realizar la transformación sobre su entrada `x` que en este caso será un par (imagen,máscara). Además el constructor de la clase que vamos a definir recibirá como parámetro las transformaciones a aplicar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q07PL4xu31hw"
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    OneOf,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    HorizontalFlip,\n",
    "    Rotate,\n",
    "    Transpose,\n",
    "    CLAHE,\n",
    "    ShiftScaleRotate\n",
    ")\n",
    "\n",
    "class SegmentationAlbumentationsTransform(ItemTransform):\n",
    "    split_idx = 0\n",
    "    \n",
    "    def __init__(self, aug): \n",
    "        self.aug = aug\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "        aug = self.aug(image=np.array(img), mask=np.array(mask))\n",
    "        return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJA6B3k431hx"
   },
   "source": [
    "En nuestro caso vamos a utilizar solo flips horizontales, rotaciones, y una operación que aplica una pequeña distorsión a la imagen. Dichas transformaciones se aplicarán de manera secuencia y de manera aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SgtwxAb31hx"
   },
   "outputs": [],
   "source": [
    "transforms=Compose([HorizontalFlip(p=0.5),\n",
    "                    Rotate(p=0.40,limit=10),GridDistortion()\n",
    "                    ],p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmR6kR8V31hx"
   },
   "source": [
    "Por último construimos un objeto de la clase definida anteriormente.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kvh6WIH931hx"
   },
   "outputs": [],
   "source": [
    "transformPipeline=SegmentationAlbumentationsTransform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOMm6Q0431hx"
   },
   "source": [
    "También va a ser necesario realizar una transformación adicional sobre las máscaras. Las máscaras contienen píxeles con 7 valores distintos (255: grape, 150: leaves, 76: pole, 74: pole, 29: wood, 25: wood, 0: background). Como vamos a trabajar únicamente con las clases grape y background, los píxeles del resto de clases deberán estar a 0 (es decir los vamos a considerar como background). Además, los números de las clases deben ser 0,1,2,... Es por esto que es necesario cambiar todos los píxeles con valor 255 a valor 1. Para realizar estas transformaciones definimos la siguiente clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eoljt50Z31hy"
   },
   "outputs": [],
   "source": [
    "class TargetMaskConvertTransform(ItemTransform):\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "        \n",
    "        #Convert to array\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        mask[mask!=255]=0\n",
    "        # Change 255 for 1\n",
    "        mask[mask==255]=1\n",
    "        \n",
    "        \n",
    "        # Back to PILMask\n",
    "        mask = PILMask.create(mask)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXqCVrFN31hy"
   },
   "source": [
    "## Dataloader\n",
    "\n",
    "Ya estamos listos para definir nuestro `DataBlock` y seguidamente nuestro `DataLoader`. Nuestro `DataBlock` se define del siguiente modo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6-G5eQL31hy"
   },
   "outputs": [],
   "source": [
    "trainDB = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n",
    "                   get_items=partial(get_image_files,folders=['train']),\n",
    "                   get_y=get_y_fn,\n",
    "                   splitter=RandomSplitter(valid_pct=0.2),\n",
    "                   item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline],\n",
    "                   batch_tfms=Normalize.from_stats(*imagenet_stats)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UdIsjWa31hy"
   },
   "source": [
    "Vamos a explicar cada una de las componentes anteriores:\n",
    "- `blocks=(ImageBlock, MaskBlock(codes))`. En este caso tenemos que la entrada de nuestro modelo va a ser una imagen (representada mediante un `ImageBlock`) y su salida es una máscara (representado mediante `MaskBlock`) cuyos posibles valores son aquellos proporcionados por la lista de clases almacenada en la variable `codes`.\n",
    "- `get_items=partial(get_image_files,folders=['train'])`. El parámetro `get_items` sirve para indicar cómo cargar los datos de nuestro dataset. Para esto vamos a usar la función `get_image_files` que devuelve los paths de las imágenes que se encuentran dentro de la carpeta `folders` (en nuestro caso la carpeta `train`). \n",
    "- `get_y=get_y_fn`. El parámetro `get_y` sirve para indicar cómo obtener la anotación asociada con una entrada (recordar que una entrada va a ser una imagen definida a partir de su path). Para esto tenemos la función `get_y_fn` definida anteriormente. \n",
    "- `splitter=RandomSplitter(valid_pct=0.2)`. Como siempre debemos partir nuestro dataset para tener un conjunto de validación de cara a seleccionar nuestros hiperparámetros. En este caso partimos el conjunto de entrenamiento usando un porcentaje 80/20.\n",
    "- `item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline]`. En el parámetro `item_tfms` indicamos las transformaciones que vamos a aplicar a nuestras imágenes y sus correspondientes máscaras. Además de las explicadas anteriormente vamos a reescalar las imágenes al tamaño 480x640.\n",
    "- `batch_tfms=Normalize.from_stats(*imagenet_stats)`. En el parámetro `batch_tfms` indicamos las transformaciones que se realizan a nivel de batch. En este caso como en nuestro modelo utilizaremos un backbone preentrenado en ImageNet debemos normalizar las imágenes para que tengan la escala de esas imágenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qIF1jo431hy"
   },
   "source": [
    "Con las explicaciones anteriores en sencillo comprender como definimos el siguiente `DataBlock` que nos servirá para evaluar nuestros modelos en el conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUqsjdel31hz"
   },
   "outputs": [],
   "source": [
    "testDB = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n",
    "                   get_items=partial(get_image_files,folders=['train','test']),\n",
    "                   get_y=get_y_fn,\n",
    "                   splitter=FuncSplitter(ParentSplitter),\n",
    "                   item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline],\n",
    "                   batch_tfms=Normalize.from_stats(*imagenet_stats)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pfqBdBD31hz"
   },
   "source": [
    "Ahora ya podemos definir nuestros `Dataloaders` indicando el path donde se encuentran las imágenes y el batch size que vamos a utilizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Pj5BbJy31hz"
   },
   "outputs": [],
   "source": [
    "bs = 4\n",
    "trainDLS = trainDB.dataloaders(path_images,bs=bs)\n",
    "testDLS = testDB.dataloaders(path_images,bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1atspFbO31h0"
   },
   "source": [
    "Como siempre es conveniente mostrar un batch para comprobar que se están cargando los datos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds0uKgkE31h0"
   },
   "outputs": [],
   "source": [
    "trainDLS.show_batch(vmin=0,vmax=1,figsize=(12, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWyBcVdx31h0"
   },
   "source": [
    "## Definición de modelo\n",
    "\n",
    "Ya podemos definir nuestro modelo y entrenarlo como hemos hecho en prácticas anteriores. Para ello vamos a crear un `Learner` mediante la función `unet_learner` a la cual le tenemos que proporcionar el `DataLoader` el backbone que vamos a utilizar (en este caso usaremos un modelo Resnet-18) y las métricas [Dice](https://docs.fast.ai/metrics.html#Dice) y [Jaccard](https://docs.fast.ai/metrics.html#JaccardCoeff) que emplearemos para evaluar nuestro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_szIGJbv31h0"
   },
   "outputs": [],
   "source": [
    "learn = unet_learner(trainDLS,resnet18,metrics=[Dice(),JaccardCoeff()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aPTM-li31h1"
   },
   "source": [
    "Por último entrenamos nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYcQpeMF31h1"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20,3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j6PDovD31h1"
   },
   "source": [
    "Una vez entrenado el modelo lo vamos a guardar para usarlo posteriormente. Lo primero que hacemos es extraer el modelo del `Learner` y caragarlo en la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBJK0Go731h2"
   },
   "outputs": [],
   "source": [
    "aux=learn.model\n",
    "aux=aux.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPTWES9O31h2"
   },
   "source": [
    "Ahora vamos a guardarlo, para lo cual es necesario cargar una imagen que le servirá como referencia para realizar las transformaciones necesarias. Para ello es necesario normalizar la imagen para que sigan el estándar de ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC4IgozK31h2"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "img = PILImage.create(path_images/'train/color_206.jpg')\n",
    "transformer=transforms.Compose([transforms.Resize((480,640)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                                    [0.485, 0.456, 0.406],\n",
    "                                                    [0.229, 0.224, 0.225])])\n",
    "img=transformer(img).unsqueeze(0)\n",
    "img=img.cpu()\n",
    "\n",
    "traced_cell=torch.jit.trace(aux, (img))\n",
    "traced_cell.save(\"unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3077xYqX31h2"
   },
   "source": [
    "## Evaluando el modelo\n",
    "\n",
    "Al igual que vimos para los modelos de clasificación, la métrica mostrada durante el proceso de entrenamiento se refiere al conjunto de validación, mientras que nos interesa saber el resultado obtenido para el conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTtuLFmH31h2"
   },
   "source": [
    "Para ello debemos modificar el dataloader del objeto `Learn` que hemos entrenado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1hYzealQYYC"
   },
   "outputs": [],
   "source": [
    "learn.dls = testDLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E52_ZXIO31h3"
   },
   "source": [
    "Por último evaluamos nuestro modelo usando el método `validate()`. En este caso el método `validate()` devuelve tres valores, el valor de la pérdida, y el valor de las métricas definidas anteriormente con respecto al conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms4LXdZ0LUV_"
   },
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PywAh0fP31h4"
   },
   "source": [
    "## Inferencia\n",
    "\n",
    "Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar el modelo. El modelo inicialmente lo cargaremos en la CPU, pero posteriormente si hay una GPU disponible la usaremos para inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eHIEmkfPgZq"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = torch.jit.load(\"unet.pth\")\n",
    "model = model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yzCcnuP31h4"
   },
   "source": [
    "El siguiente paso es cargar la imagen, para lo que usaremos la librería `PIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq044_u5MHHP"
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TunuHhauMH_e"
   },
   "outputs": [],
   "source": [
    "img = PIL.Image.open('dataset/Images/test/color_154.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL61ysjc31h5"
   },
   "source": [
    "La siguiente instrucción permite mostrar la imagen que acabamos de cargar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGrZkmcDMJXI"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdHpiDHs31h5"
   },
   "source": [
    "Ya estaríamos listos para relizar las predicciones sobre la imagen. Sin embargo, cabe recordar que primero debemos reescalar las imágenes y normalizarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g-twflKMFo9"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "def transform_image(image):\n",
    "    my_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])])\n",
    "    image_aux = image\n",
    "    return my_transforms(image_aux).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya9jjF4O31h6"
   },
   "source": [
    "El siguiente paso consiste en transformar la imagen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWMTK4xoMKnG"
   },
   "outputs": [],
   "source": [
    "image = transforms.Resize((480,640))(img)\n",
    "tensor = transform_image(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okcqk8Tq31h6"
   },
   "source": [
    "Ahora ya podemos realizar pasarle el objeto construido anteriormente al modelo para realizar la predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nEvXp7dMMZI"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(tensor)\n",
    "\n",
    "outputs = torch.argmax(outputs,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0U4XRRs31h6"
   },
   "source": [
    "Ahora almacenamos el resultado en un array y convertimos el índice asociado con la clase grape (que era 1) al valor 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd6j6v0sMN3O"
   },
   "outputs": [],
   "source": [
    "mask = np.array(outputs.cpu())\n",
    "mask[mask==1]=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBxeC3WS31h7"
   },
   "source": [
    "La predicción devuelta por el modelo es un vector de tamaño 480x640 por lo que tendremos que ponerla en forma de matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GA83FFiV31h8"
   },
   "outputs": [],
   "source": [
    "mask=np.reshape(mask,(480,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVbkTsNt31h8"
   },
   "source": [
    "Con esto ya podemos mostrar la máscara generada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZQ7XLj931h8"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(mask.astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxtBiyOA7kTO"
   },
   "source": [
    "Podemos compararla con la máscara real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo0EQpO46gb2"
   },
   "outputs": [],
   "source": [
    "PIL.Image.open('dataset/Labels/test/gt_154.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZy86xNI7q4M"
   },
   "source": [
    "Como vemos el modelo se aproxima bastante, pero la segmentación no es excesivamente buena. En la práctica veremos cómo crear mejores modelos. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practica3Instrucciones.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
