{
  
    
        "post0": {
            "title": "Modelos de clasificación de texto",
            "content": "En este notebook se muestra cómo crear un modelo de clasificación de texto usando una variante de una red LSTM implementada en FastAI. . Para esta práctica es necesario el uso de GPU, así que recuerda activar esta opción en Colab. . Librer&#237;as . Comenzamos actualizando la librería FastAI y descargando la librería datasets de HuggingFace. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -&gt; Reiniciar Entorno de ejecución). . !pip install fastai -Uqq !pip install datasets -Uqq . Cargamos a continuación las librerías que necesitaremos en esta práctica que son la parte de procesado de lenguaje natural de la librería fastAI, la librería pandas, y la funcionalidad para cargar datasets de HuggingFace. . import pandas as pd from fastai.text.all import * from datasets import load_dataset . Dataset . Para este ejemplo vamos a usar el dataset Gutenberg Poem Dataset, un dataset para detectar sentimientos en poemas (negativos, positivos, sin impacto, mezcla de positivo y negativo). . Descarga el dataset usando el siguiente comando. . poem_sentiment_dataset = load_dataset(&quot;poem_sentiment&quot;) . Using custom data configuration default Reusing dataset poem_sentiment (/root/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099) . A continuación mostramos el dataset que hemos descargado. . poem_sentiment_dataset . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;verse_text&#39;, &#39;label&#39;], num_rows: 892 }) validation: Dataset({ features: [&#39;id&#39;, &#39;verse_text&#39;, &#39;label&#39;], num_rows: 105 }) test: Dataset({ features: [&#39;id&#39;, &#39;verse_text&#39;, &#39;label&#39;], num_rows: 104 }) }) . Podemos ver que el dataset es una estructura DatasetDict que puede verse como un diccionario. El diccionario tiene tres claves que son train, validation y test que indican respectivamente los conjuntos de entrenamiento, validación y test (estas claves pueden variar dependiendo del dataset). Cada uno de estos subconjuntos es un Dataset que puede verse como una lista. Podemos ver por ejemplo la primera frase del conjunto de entrenamiento del siguiente modo. . poem_sentiment_dataset[&quot;train&quot;][0] . {&#39;id&#39;: 0, &#39;label&#39;: 1, &#39;verse_text&#39;: &#39;with pale blue berries. in these peaceful shades--&#39;} . Podemos ver también una descripción del dataset usando el atributo info. . print(poem_sentiment_dataset[&quot;train&quot;].info) . DatasetInfo(description=&#39;Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems. n&#39;, citation=&#39;@misc{sheng2020investigating, n title={Investigating Societal Biases in a Poetry Composition System}, n author={Emily Sheng and David Uthus}, n year={2020}, n eprint={2011.02686}, n archivePrefix={arXiv}, n primaryClass={cs.CL} n} n&#39;, homepage=&#39;https://github.com/google-research-datasets/poem-sentiment&#39;, license=&#39;&#39;, features={&#39;id&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;verse_text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;label&#39;: ClassLabel(num_classes=4, names=[&#39;negative&#39;, &#39;positive&#39;, &#39;no_impact&#39;, &#39;mixed&#39;], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=&#39;poem_sentiment&#39;, config_name=&#39;default&#39;, version=1.0.0, splits={&#39;train&#39;: SplitInfo(name=&#39;train&#39;, num_bytes=48667, num_examples=892, dataset_name=&#39;poem_sentiment&#39;), &#39;validation&#39;: SplitInfo(name=&#39;validation&#39;, num_bytes=5802, num_examples=105, dataset_name=&#39;poem_sentiment&#39;), &#39;test&#39;: SplitInfo(name=&#39;test&#39;, num_bytes=5601, num_examples=104, dataset_name=&#39;poem_sentiment&#39;)}, download_checksums={&#39;https://raw.githubusercontent.com/google-research-datasets/poem-sentiment/master/data//train.tsv&#39;: {&#39;num_bytes&#39;: 40567, &#39;checksum&#39;: &#39;09503a850796736a8e63504ae2c8de983f57dceebc2342ea93a7f7ad06af56d0&#39;}, &#39;https://raw.githubusercontent.com/google-research-datasets/poem-sentiment/master/data//dev.tsv&#39;: {&#39;num_bytes&#39;: 4747, &#39;checksum&#39;: &#39;0770bd7b80e04c92da82e78f9f93ce35d38b334b4830fca2b9b4d7e174f152e5&#39;}, &#39;https://raw.githubusercontent.com/google-research-datasets/poem-sentiment/master/data//test.tsv&#39;: {&#39;num_bytes&#39;: 4556, &#39;checksum&#39;: &#39;a406d87b62a6b7c9615468a8b66e69c027fa8f2f03a4f8f6c79df26c260de620&#39;}}, download_size=49870, post_processing_size=None, dataset_size=60070, size_in_bytes=109940) . Por último, podemos ver las características de nuestro dataset utilizando el atributo features. . poem_sentiment_dataset[&quot;train&quot;].features . {&#39;id&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;label&#39;: ClassLabel(num_classes=4, names=[&#39;negative&#39;, &#39;positive&#39;, &#39;no_impact&#39;, &#39;mixed&#39;], id=None), &#39;verse_text&#39;: Value(dtype=&#39;string&#39;, id=None)} . Con el comando anterior podemos ver las cuatro clases de nuestro dataset, y los índices que les corresponden. . Carga de datos . Cargamos a continuación el dataset en distintos dataframes de pandas (el formato que puede leer la librería de FastAI). . train_df = poem_sentiment_dataset[&quot;train&quot;].to_pandas() valid_df = poem_sentiment_dataset[&quot;validation&quot;].to_pandas() test_df = poem_sentiment_dataset[&quot;test&quot;].to_pandas() . Podemos ver el contenido del dataset usando el siguiente comando. . train_df . id verse_text label . 0 0 | with pale blue berries. in these peaceful shades-- | 1 | . 1 1 | it flows so long as falls the rain, | 2 | . 2 2 | and that is why, the lonesome day, | 0 | . 3 3 | when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals, | 3 | . 4 4 | of inward strife for truth and liberty. | 3 | . ... ... | ... | ... | . 887 887 | to his ears there came a murmur of far seas beneath the wind | 2 | . 888 888 | the one good man in the world who knows me, -- | 1 | . 889 889 | faint voices lifted shrill with pain | 0 | . 890 890 | an&#39;, fust you knowed on, back come charles the second; | 2 | . 891 891 | in the wild glens rough shepherds will deplore | 0 | . 892 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Del dataset nos interesan dos campos: verse_text (que contiene el poema) y label (que contiene el sentimiento). . train_df[&#39;verse_text&#39;] . 0 with pale blue berries. in these peaceful shades-- 1 it flows so long as falls the rain, 2 and that is why, the lonesome day, 3 when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals, 4 of inward strife for truth and liberty. ... 887 to his ears there came a murmur of far seas beneath the wind 888 the one good man in the world who knows me, -- 889 faint voices lifted shrill with pain 890 an&#39;, fust you knowed on, back come charles the second; 891 in the wild glens rough shepherds will deplore Name: verse_text, Length: 892, dtype: object . train_df[&#39;label&#39;] . 0 1 1 2 2 0 3 3 4 3 .. 887 2 888 1 889 0 890 2 891 0 Name: label, Length: 892, dtype: int64 . Juntamos ahora nuestro conjunto de entrenamiento y de validación. Para poder diferenciarlos vamos a añadir una columna a cada uno de ellos para indicar al conjunto al que pertenecen. La columna set va a tener dos valores: True si pertenece al conjunto de validación y False si pertenece al conjunto de test. . train_df[&#39;set&#39;]=False valid_df[&#39;set&#39;]=True . También eliminamos la columna id que no va a ser necesaria. . train_df = train_df.drop(columns=[&#39;id&#39;],axis=1) valid_df = valid_df.drop(columns=[&#39;id&#39;],axis=1) . Ahora juntamos ambos datasets. . train_valid_df = pd.concat([train_df,valid_df]) . Por último, hay que renombrar la columna verse_text por text (esto es una restricción de FastAI). . train_valid_df = train_valid_df.rename(columns={&quot;verse_text&quot;: &quot;text&quot;}) train_valid_df . text label set . 0 with pale blue berries. in these peaceful shades-- | 1 | False | . 1 it flows so long as falls the rain, | 2 | False | . 2 and that is why, the lonesome day, | 0 | False | . 3 when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals, | 3 | False | . 4 of inward strife for truth and liberty. | 3 | False | . ... ... | ... | ... | . 100 said my companion, &#39;i will show you soon | 2 | True | . 101 but god said | 2 | True | . 102 but if thou do thy best | 2 | True | . 103 so generous to me. farewell, friend, since friend | 1 | True | . 104 let me all risk, and leave the deep heart dumb | 0 | True | . 997 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Entrenando un modelo . El proceso para entrenar un modelo es el mismo que hemos visto para los modelos de clasificación de imágenes y podemos usar toda la funcionalidad vista hasta ahora. Comenzamos definiendo un DataBlock que se creará a partir de nuestro dataframe df. Tendremos que definir también una función que nos indique cuándo un poema es del conjunto de entrenamiento o del de validación. . sentiment_clas = DataBlock( blocks=(TextBlock.from_df(&#39;text&#39;), CategoryBlock), # La entrada del modelo es texto, y la salida una clase get_x=ColReader(&#39;text&#39;), # Indicamos donde estará el texto dentro del dataframe get_y=ColReader(&#39;label&#39;), # Indicamos cómo extraer la clase del dataframe splitter=ColSplitter(&#39;set&#39;) # Partimos el dataset en entrenamiento y validación ) . Ahora definimos nuestro dataloader a partir del DataBlock que acabamos de crear (esto le puede costar unos segundos). . dls = sentiment_clas.dataloaders(train_valid_df, bs=64) . Podemos mostrar un batch de nuestro dataloader. . dls.show_batch(max_n=2) . text category . 0 xxbos when i xxunk the xxunk xxunk of xxunk , and the xxunk of mighty xxunk , i do not envy the xxunk , | 3 | . 1 xxbos till , xxunk &#39; xxunk i know , there xxunk xxunk an xxunk xxunk i could lay my xxunk &#39; on , | 2 | . Pasamos ahora a crear nuestro learner usando el método text_classifier_learner al que pasamos como arquitectura de red la arquitectura AWD_LSTM, además aplicamos dropout a nuestro modelo. . callbacks = [ShowGraphCallback(), SaveModelCallback()] learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy,cbs=callbacks).to_fp16() . Ahora podemos utilizar toda la funcionalidad que ya vimos para clasificación de imágenes. Por ejemplo, podemos buscar un learning rate adecuado para entrenar nuestro modelo. . learn.lr_find() . SuggestedLRs(valley=0.002511886414140463) . Y a continuación aplicar fine tuning. . learn.fine_tune(10, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 1.432006 | 1.178530 | 0.638095 | 00:02 | . Better model found at epoch 0 with valid_loss value: 1.1785303354263306. . epoch train_loss valid_loss accuracy time . 0 | 1.194728 | 1.150354 | 0.638095 | 00:02 | . 1 | 1.108590 | 1.034699 | 0.657143 | 00:02 | . 2 | 1.015784 | 1.019889 | 0.647619 | 00:02 | . 3 | 0.909297 | 1.137827 | 0.685714 | 00:02 | . 4 | 0.792113 | 1.079932 | 0.666667 | 00:02 | . 5 | 0.691815 | 1.335006 | 0.628571 | 00:02 | . 6 | 0.600908 | 1.133648 | 0.647619 | 00:02 | . 7 | 0.505813 | 1.160434 | 0.666667 | 00:02 | . 8 | 0.426953 | 1.188708 | 0.676190 | 00:02 | . 9 | 0.370063 | 1.186051 | 0.685714 | 00:02 | . Better model found at epoch 0 with valid_loss value: 1.1503543853759766. . Better model found at epoch 1 with valid_loss value: 1.0346990823745728. Better model found at epoch 2 with valid_loss value: 1.0198886394500732. . Ahora podemos usar nuestro modelo para predecir la clase de una nueva frase. . learn.predict(&#39;with pale blue berries. in these peaceful shades--.&#39;) . (&#39;1&#39;, TensorText(1), TensorText([0.1539, 0.4554, 0.1342, 0.2565])) . Por último, podemos validar nuestro modelo en el conjunto de test, para lo cuál hay que combinar los dataframes y construir un nuevo dataloader. . test_df[&#39;set&#39;]=True test_df = test_df.drop(columns=[&#39;id&#39;],axis=1) train_test_df = pd.concat([train_df,test_df]) train_test_df = train_test_df.rename(columns={&quot;verse_text&quot;: &quot;text&quot;}) dls_test = sentiment_clas.dataloaders(train_test_df, bs=64) . Modificamos ahora el dataloader de nuestro learner, y procedemos a validar. . learn.dls = dls_test learn.validate() . Better model found at epoch 0 with valid_loss value: 0.5480769276618958. . (#2) [1.1024993658065796,0.5480769276618958] . Como podemos ver nuestro modelo tiene una accuracy del 66%. En las próximas prácticas veremos cómo mejorarlo. .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/practica/2022/03/22/practica6-clasificacion-texto-basica.html",
            "relUrl": "/practica/2022/03/22/practica6-clasificacion-texto-basica.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Autoencoder",
            "content": "En este notebook se muestra cómo se puede definir un variational autoencoder. Para ello vamos a utilizar la librería Keras y el dataset MNIST. Pero vamos a ver primero un autoencoder básico. . En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes). . Un autoencoder b&#225;sico . El dataset MNIST es un dataset ampliamente utilizado para probar algoritmos de aprendizaje automático. El problema que se intenta resolver con el dataset MNIST consiste en clasificar imágenes en escala de grises de dígitos manuscritos (de tamaño 28x28) en 10 categorías (del 0 al 9). En nuestro caso no vamos a crear un clasificador para el dataset de MNIST sino que lo vamos a utilizar para mostrar cómo podemos usar un auto-encoder para eliminar el ruido de las imágenes. . Comenzamos cargando las librerías necesarias. . import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras import backend as K import warnings warnings.filterwarnings(&quot;ignore&quot;) import matplotlib.pyplot as plt . Dataset . Para entrenar el autoencoder vamos a usar el dataset de MNIST donde los píxeles estarán normalizados entre 0 y 1. . En este caso no nos interesan las etiquetas del dataset, solo las imágenes. . (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 1s 0us/step 11501568/11490434 [==============================] - 1s 0us/step . Normalizamos el dataset y lo preparamos para poder alimentar al autoencoder. . x_train = x_train.astype(&#39;float32&#39;) / 255. x_test = x_test.astype(&#39;float32&#39;) / 255. x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) . Autoencoder . Pasamos ahora a definir la arquitectura de nuestro autoencoder. Para ello lo primero que debemos hacer es definir la forma que tendrán los datos de entrada de nuestro autoencoder. El dataset MNIST consta de imágenes de tamaño $28 times 28$ en escala de grises, por lo que solo tienen un canal. . input_img = layers.Input(shape=(28,28,1)) . Recordar que un autoencoder consta de un encoder y un decoder. Definimos a continuación nuestro encoder que va a constar de una pila de capas de convolución y de max pooling. Al aplicar el proceso de encoding llegamos a una reprentación final de tamaño (4,4,8) es decir 128 dimensiones, es decir hemos reducido casi a una sexta parte la codificación de nuestras imágenes. . x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(input_img) # Capa de max pooling con filtro de tamaño 2x2 y aplicando padding x = layers.MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) encoded = layers.MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) . El decoder se define mediante una pila de capas de convolución y de upsampling (capas con la función inversa que las de pooling). Notar que la entrada de la primera capa del decoder es la salida del encoder. Notar que la arquitectura es simétrica a la del encoder. . x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(encoded) # Capa de upsampling con filtro de tamaño 2x2 y aplicando padding x = layers.UpSampling2D((2, 2))(x) x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = layers.UpSampling2D((2, 2))(x) decoded = layers.Conv2D(1, (3, 3), activation=&#39;sigmoid&#39;, padding=&#39;same&#39;)(x) . Por último definimos nuestro modelo de autoencoder y lo compilamos. En Keras es necesario compilar un modelo para fijar el optimizador que se utilizará para entrenarlo (en este caso ADAM que es una variante del descenso de gradiente) y la función de pérdida (en este caso la binary crossentropy). . autoencoder = keras.Model(input_img, decoded) autoencoder.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;) . Con la siguiente instrucción podemos mostrar la arquitectura de una red de Keras. . autoencoder.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D (None, 14, 14, 32) 0 ) conv2d_1 (Conv2D) (None, 14, 14, 32) 9248 max_pooling2d_1 (MaxPooling (None, 7, 7, 32) 0 2D) conv2d_2 (Conv2D) (None, 7, 7, 32) 9248 up_sampling2d (UpSampling2D (None, 14, 14, 32) 0 ) conv2d_3 (Conv2D) (None, 14, 14, 32) 9248 up_sampling2d_1 (UpSampling (None, 28, 28, 32) 0 2D) conv2d_4 (Conv2D) (None, 28, 28, 1) 289 ================================================================= Total params: 28,353 Trainable params: 28,353 Non-trainable params: 0 _________________________________________________________________ . También puede ser útil visualizar dicha red. . from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file=&#39;autoencoder_plot.png&#39;, show_shapes=True, show_layer_names=True) . Vamos ahora a entrenar nuestro modelo para ello usamos el método fit que está disponible para cualquier modelo de Keras. . autoencoder.fit(x_train, x_train, epochs=50, batch_size=128, validation_data=(x_test, x_test)) . Epoch 1/50 469/469 [==============================] - 19s 18ms/step - loss: 0.1155 - val_loss: 0.0767 Epoch 2/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0742 - val_loss: 0.0715 Epoch 3/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0712 - val_loss: 0.0700 Epoch 4/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0697 - val_loss: 0.0686 Epoch 5/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0688 - val_loss: 0.0679 Epoch 6/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0680 - val_loss: 0.0673 Epoch 7/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0675 - val_loss: 0.0667 Epoch 8/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0669 - val_loss: 0.0663 Epoch 9/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0666 - val_loss: 0.0662 Epoch 10/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0662 - val_loss: 0.0657 Epoch 11/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0659 - val_loss: 0.0653 Epoch 12/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0657 - val_loss: 0.0653 Epoch 13/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0655 - val_loss: 0.0649 Epoch 14/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0653 - val_loss: 0.0647 Epoch 15/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0651 - val_loss: 0.0647 Epoch 16/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0649 - val_loss: 0.0645 Epoch 17/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0648 - val_loss: 0.0644 Epoch 18/50 469/469 [==============================] - 9s 19ms/step - loss: 0.0647 - val_loss: 0.0643 Epoch 19/50 469/469 [==============================] - 8s 16ms/step - loss: 0.0645 - val_loss: 0.0645 Epoch 20/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0644 - val_loss: 0.0640 Epoch 21/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0643 - val_loss: 0.0641 Epoch 22/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0642 - val_loss: 0.0638 Epoch 23/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0641 - val_loss: 0.0638 Epoch 24/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0640 - val_loss: 0.0636 Epoch 25/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0640 - val_loss: 0.0635 Epoch 26/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0639 - val_loss: 0.0635 Epoch 27/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0638 - val_loss: 0.0635 Epoch 28/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0638 - val_loss: 0.0634 Epoch 29/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0637 - val_loss: 0.0634 Epoch 30/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0637 - val_loss: 0.0633 Epoch 31/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0636 - val_loss: 0.0632 Epoch 32/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0636 - val_loss: 0.0632 Epoch 33/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0635 - val_loss: 0.0631 Epoch 34/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0635 - val_loss: 0.0631 Epoch 35/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0634 - val_loss: 0.0630 Epoch 36/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0634 - val_loss: 0.0631 Epoch 37/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0634 - val_loss: 0.0630 Epoch 38/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0633 - val_loss: 0.0630 Epoch 39/50 469/469 [==============================] - 7s 15ms/step - loss: 0.0633 - val_loss: 0.0629 Epoch 40/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0633 - val_loss: 0.0629 Epoch 41/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0632 - val_loss: 0.0629 Epoch 42/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0632 - val_loss: 0.0629 Epoch 43/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0632 - val_loss: 0.0629 Epoch 44/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0632 - val_loss: 0.0628 Epoch 45/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0631 - val_loss: 0.0627 Epoch 46/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0631 - val_loss: 0.0628 Epoch 47/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0631 - val_loss: 0.0627 Epoch 48/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0631 - val_loss: 0.0628 Epoch 49/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0630 - val_loss: 0.0626 Epoch 50/50 469/469 [==============================] - 7s 14ms/step - loss: 0.0630 - val_loss: 0.0627 . &lt;keras.callbacks.History at 0x7f0a900f1610&gt; . Vamos a mostrar la reconstrucción de algunos de los dígitos. Al ejecutar la siguiente celda, la primera fila muestra los dígitos originales y la segunda los reconstruidos. . decoded_imgs = autoencoder.predict(x_test) n = 10 plt.figure(figsize=(20, 4)) for i in range(1,n): ax = plt.subplot(2, n, i) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) ax = plt.subplot(2, n, i + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() . Aplicaci&#243;n a la eliminaci&#243;n de ruido . Vamos a poner ahora a nuestro encoder a trabajar en el problema de la eliminación de ruido. Esto va a ser tan sencillo como entrenar nuestro autoencoder para mapear dígitos con ruido a imágenes limpias. . Para ello, el primer paso es construir nuestro dataset con ruido aplicando un ruido Gaussiano a las imágenes, y luego limitando los valores al rango de 0 a 1. . En primer lugar procedemos a añadir ruido a las imágenes del dataset de MNIST. . noise_factor = 0.5 x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) x_train_noisy = np.clip(x_train_noisy, 0., 1.) x_test_noisy = np.clip(x_test_noisy, 0., 1.) . Podemos ver a continuación algunas de las imágenes a las que se les ha añadido el ruido. . n = 10 plt.figure(figsize=(20, 2)) for i in range(1, n + 1): ax = plt.subplot(1, n, i) plt.imshow(x_test_noisy[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() . Autoencoder . Pasamos ahora a definir la arquitectura de nuestro autoencoder. Para ello lo primero que debemos hacer es definir la forma que tendrán los datos de entrada de nuestro autoencoder. El dataset MNIST consta de imágenes de tamaño $28 times 28$ en escala de grises, por lo que solo tienen un canal. . input_img = layers.Input(shape=(28,28,1)) . Recordar que un autoencoder consta de un encoder y un decoder. Definimos a continuación nuestro encoder que va a constar de una pila de capas de convolución y de maxpooling. . x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(input_img) x = layers.MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) encoded = layers.MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) . El decoder se define mediante una pila de capas de convolución y de upsampling (capas con la función inversa que las de pooling). Notar que la entrada de la primera capa del decoder es la salida del encoder; además, al igual que antes la arquitectura es simétrica al encoder. . x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(encoded) x = layers.UpSampling2D((2, 2))(x) x = layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = layers.UpSampling2D((2, 2))(x) decoded = layers.Conv2D(1, (3, 3), activation=&#39;sigmoid&#39;, padding=&#39;same&#39;)(x) . Por último definimos nuestro modelo de autoencoder y lo compilamos. . autoencoder_noise = keras.Model(input_img, decoded) autoencoder_noise.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;) . Con la siguiente instrucción podemos mostrar la arquitectura del nuevo autoencoder. . autoencoder_noise.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 28, 28, 1)] 0 conv2d_5 (Conv2D) (None, 28, 28, 32) 320 max_pooling2d_2 (MaxPooling (None, 14, 14, 32) 0 2D) conv2d_6 (Conv2D) (None, 14, 14, 32) 9248 max_pooling2d_3 (MaxPooling (None, 7, 7, 32) 0 2D) conv2d_7 (Conv2D) (None, 7, 7, 32) 9248 up_sampling2d_2 (UpSampling (None, 14, 14, 32) 0 2D) conv2d_8 (Conv2D) (None, 14, 14, 32) 9248 up_sampling2d_3 (UpSampling (None, 28, 28, 32) 0 2D) conv2d_9 (Conv2D) (None, 28, 28, 1) 289 ================================================================= Total params: 28,353 Trainable params: 28,353 Non-trainable params: 0 _________________________________________________________________ . También puede ser útil visualizar dicha red. . plot_model(autoencoder_noise, to_file=&#39;autoencoder_noise_plot.png&#39;, show_shapes=True, show_layer_names=True) . Vamos ahora a entrenar nuestro modelo para ello usamos el método fit que está disponible para cualquier modelo de Keras. . autoencoder_noise.fit(x_train_noisy, x_train, epochs=100, batch_size=128, validation_data=(x_test_noisy, x_test)) . Epoch 1/100 469/469 [==============================] - 8s 15ms/step - loss: 0.1726 - val_loss: 0.1164 Epoch 2/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1134 - val_loss: 0.1092 Epoch 3/100 469/469 [==============================] - 6s 14ms/step - loss: 0.1083 - val_loss: 0.1059 Epoch 4/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1056 - val_loss: 0.1041 Epoch 5/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1037 - val_loss: 0.1021 Epoch 6/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1023 - val_loss: 0.1009 Epoch 7/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1011 - val_loss: 0.0998 Epoch 8/100 469/469 [==============================] - 7s 14ms/step - loss: 0.1001 - val_loss: 0.0990 Epoch 9/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0993 - val_loss: 0.0982 Epoch 10/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0987 - val_loss: 0.0979 Epoch 11/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0983 - val_loss: 0.0976 Epoch 12/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0979 - val_loss: 0.0973 Epoch 13/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0975 - val_loss: 0.0967 Epoch 14/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0971 - val_loss: 0.0972 Epoch 15/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0969 - val_loss: 0.0963 Epoch 16/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0966 - val_loss: 0.0964 Epoch 17/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0964 - val_loss: 0.0958 Epoch 18/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0962 - val_loss: 0.0955 Epoch 19/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0959 - val_loss: 0.0957 Epoch 20/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0958 - val_loss: 0.0954 Epoch 21/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0957 - val_loss: 0.0952 Epoch 22/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0956 - val_loss: 0.0954 Epoch 23/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0954 - val_loss: 0.0951 Epoch 24/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0953 - val_loss: 0.0948 Epoch 25/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0952 - val_loss: 0.0948 Epoch 26/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0951 - val_loss: 0.0953 Epoch 27/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0950 - val_loss: 0.0948 Epoch 28/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0949 - val_loss: 0.0947 Epoch 29/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0949 - val_loss: 0.0945 Epoch 30/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0948 - val_loss: 0.0944 Epoch 31/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0947 - val_loss: 0.0944 Epoch 32/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0947 - val_loss: 0.0945 Epoch 33/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0946 - val_loss: 0.0943 Epoch 34/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0945 - val_loss: 0.0945 Epoch 35/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0945 - val_loss: 0.0944 Epoch 36/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0945 - val_loss: 0.0943 Epoch 37/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0944 - val_loss: 0.0941 Epoch 38/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0944 - val_loss: 0.0941 Epoch 39/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0943 - val_loss: 0.0942 Epoch 40/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0942 - val_loss: 0.0941 Epoch 41/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0943 - val_loss: 0.0942 Epoch 42/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0942 - val_loss: 0.0940 Epoch 43/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0942 - val_loss: 0.0939 Epoch 44/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0941 - val_loss: 0.0941 Epoch 45/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0941 - val_loss: 0.0938 Epoch 46/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0941 - val_loss: 0.0941 Epoch 47/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0941 - val_loss: 0.0941 Epoch 48/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0940 - val_loss: 0.0939 Epoch 49/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0940 - val_loss: 0.0938 Epoch 50/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0939 - val_loss: 0.0939 Epoch 51/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0940 - val_loss: 0.0938 Epoch 52/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0939 - val_loss: 0.0939 Epoch 53/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0938 - val_loss: 0.0939 Epoch 54/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0938 - val_loss: 0.0938 Epoch 55/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0939 - val_loss: 0.0937 Epoch 56/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0938 - val_loss: 0.0938 Epoch 57/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0938 - val_loss: 0.0939 Epoch 58/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0938 - val_loss: 0.0939 Epoch 59/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0937 Epoch 60/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0937 Epoch 61/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0937 Epoch 62/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0936 Epoch 63/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0936 Epoch 64/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0937 - val_loss: 0.0938 Epoch 65/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0937 - val_loss: 0.0937 Epoch 66/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0937 Epoch 67/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0939 Epoch 68/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0936 Epoch 69/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0936 - val_loss: 0.0936 Epoch 70/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0936 Epoch 71/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0938 Epoch 72/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0936 - val_loss: 0.0938 Epoch 73/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0936 Epoch 74/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0936 Epoch 75/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0937 Epoch 76/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0938 Epoch 77/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0937 Epoch 78/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0935 Epoch 79/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0937 Epoch 80/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0935 - val_loss: 0.0936 Epoch 81/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0934 - val_loss: 0.0934 Epoch 82/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0935 - val_loss: 0.0940 Epoch 83/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0942 Epoch 84/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0936 Epoch 85/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0934 - val_loss: 0.0934 Epoch 86/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0934 - val_loss: 0.0936 Epoch 87/100 469/469 [==============================] - 7s 15ms/step - loss: 0.0934 - val_loss: 0.0934 Epoch 88/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0934 Epoch 89/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0935 Epoch 90/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0934 Epoch 91/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0935 Epoch 92/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 93/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 94/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0934 - val_loss: 0.0938 Epoch 95/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 96/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 97/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 98/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 99/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0935 Epoch 100/100 469/469 [==============================] - 7s 14ms/step - loss: 0.0933 - val_loss: 0.0934 . &lt;keras.callbacks.History at 0x7f0a234423d0&gt; . Por último podemos predecir usando el modelo entrenado con las imágenes de test: . decoded_imgs = autoencoder_noise.predict(x_test_noisy) . Y a continuación mostrar el resultado obtenido. . n = 10 plt.figure(figsize=(20, 4)) for i in range(1,n): ax = plt.subplot(2, n, i) plt.imshow(x_test_noisy[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) ax = plt.subplot(2, n, i + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() . Variational autoencoder . Como hemos visto en teoría un autoencoder toma una imagen, la mapea a un espacio vectorial subyacente por medio de un encoder, y seguidamente la decodifica a una salida con el tamaño de la imagen original. En la práctica, los autoencoders no son especialmente útiles, y han sido reemplazados por los variational autoencoders (o VAEs). . Un VAE en lugar de comprimir la imagen en un vector fijo en el espacio subyacente, la convierte a los parámetros de una distribución estadística (representada mediante una media y una varianza). Esencialmente, esto significa que asumimos que la imagen original ha sido generada mediante un proceso estadístico, y que la aleatoriedad de dicho proceso debe ser tomada en cuenta a la hora de proceso de codificación y decodificación. . Un VAE usa los parámetros de media y varianza para tomar una muestra aleatoria de un elemento de la distribución, y decodifica dicho elemento de vuelta. Esto hace que mejore la robusted y fuerza a que el espacio subyacente obtenga representaciones significativas (notar que cualquier muestra de la distribución tiene que ser decodificada a una salida valida). . Desde el punto de vista técnico, un VAE funciona del siguiente modo: . Un encoder convierte la entrada en dos parámetros de un espacio subyacente de representaciones que denotaremos por z_mean y z_log_var. | Tomamos una muestra aleatoria z de la distribución normal que asumimos que genera la imagen de entrada mediante la fórmula z = z_mean + exp(z_log_var)*epsilon donde epsilon es un valor aleatorio pequeño. | La muestra z se decodifica. Al tomar epsilon de manera aleatoria y con valor pequeño, el proceso asegura que cada punto que está cerca de la localización subyacente de la imagen puede ser decodificado a algo similar a la imagen de entrada. | Para entrenar un VAE se usan dos funciones de pérdida: una que es la función de pérdida de reconstrucción que fuerza a que las muestras decodificadas se ajusten a las entradas iniciales, y una función de pérdida de regularización que ayuda a una formación correcta de los espacios subyacentes y a que no se produzca sobreajuste. . Capa de Sampling . Lo primero que vamos a definir es una nueva capa encargada de tomar una muestra aleatoria a partir de los valores de z_mean y z_log_var. Para ello debemos definir una nueva clase que hereda de la clase Layer de Keras y definir la función call. . import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers class Sampling(layers.Layer): def call(self, inputs): z_mean, z_log_var = inputs batch = tf.shape(z_mean)[0] dim = tf.shape(z_mean)[1] epsilon = tf.keras.backend.random_normal(shape=(batch, dim)) return z_mean + tf.exp(0.5 * z_log_var) * epsilon . Encoder . El encoder va a ser similar al encoder que vimos para el autoencoder, la principal diferencia es que va a producir dos vectores de salida, z_mean y z_log_var. . latent_dim = 2 encoder_inputs = keras.Input(shape=(28, 28, 1)) x = layers.Conv2D(32, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(encoder_inputs) x = layers.Conv2D(64, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x) x = layers.Flatten()(x) x = layers.Dense(16, activation=&quot;relu&quot;)(x) z_mean = layers.Dense(latent_dim, name=&quot;z_mean&quot;)(x) z_log_var = layers.Dense(latent_dim, name=&quot;z_log_var&quot;)(x) z = Sampling()([z_mean, z_log_var]) encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=&quot;encoder&quot;) encoder.summary() . Model: &#34;encoder&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) [(None, 28, 28, 1)] 0 [] conv2d_10 (Conv2D) (None, 14, 14, 32) 320 [&#39;input_3[0][0]&#39;] conv2d_11 (Conv2D) (None, 7, 7, 64) 18496 [&#39;conv2d_10[0][0]&#39;] flatten (Flatten) (None, 3136) 0 [&#39;conv2d_11[0][0]&#39;] dense (Dense) (None, 16) 50192 [&#39;flatten[0][0]&#39;] z_mean (Dense) (None, 2) 34 [&#39;dense[0][0]&#39;] z_log_var (Dense) (None, 2) 34 [&#39;dense[0][0]&#39;] sampling (Sampling) (None, 2) 0 [&#39;z_mean[0][0]&#39;, &#39;z_log_var[0][0]&#39;] ================================================================================================== Total params: 69,076 Trainable params: 69,076 Non-trainable params: 0 __________________________________________________________________________________________________ . Decoder . Ahora podemos definir el decoder, utilizando una arquitectura simétrica al encoder. . latent_inputs = keras.Input(shape=(latent_dim,)) x = layers.Dense(7 * 7 * 64, activation=&quot;relu&quot;)(latent_inputs) x = layers.Reshape((7, 7, 64))(x) x = layers.Conv2DTranspose(64, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x) x = layers.Conv2DTranspose(32, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x) decoder_outputs = layers.Conv2DTranspose(1, 3, activation=&quot;sigmoid&quot;, padding=&quot;same&quot;)(x) decoder = keras.Model(latent_inputs, decoder_outputs, name=&quot;decoder&quot;) decoder.summary() . Model: &#34;decoder&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_4 (InputLayer) [(None, 2)] 0 dense_1 (Dense) (None, 3136) 9408 reshape (Reshape) (None, 7, 7, 64) 0 conv2d_transpose (Conv2DTra (None, 14, 14, 64) 36928 nspose) conv2d_transpose_1 (Conv2DT (None, 28, 28, 32) 18464 ranspose) conv2d_transpose_2 (Conv2DT (None, 28, 28, 1) 289 ranspose) ================================================================= Total params: 65,089 Trainable params: 65,089 Non-trainable params: 0 _________________________________________________________________ . Por último vamos a definir un nuevo modelo que une nuestro encoder y decoder definidos anteriormente, y definimos nuestra función de pérdida que va a tener en cuenta la función de pérdida de reconstrucción y la función de pérdida de regularización. Para ello debemos definir una nueva clase que herede de Model. . class VAE(keras.Model): def __init__(self, encoder, decoder, **kwargs): super(VAE, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder self.total_loss_tracker = keras.metrics.Mean(name=&quot;total_loss&quot;) self.reconstruction_loss_tracker = keras.metrics.Mean( name=&quot;reconstruction_loss&quot; ) self.kl_loss_tracker = keras.metrics.Mean(name=&quot;kl_loss&quot;) @property def metrics(self): return [ self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker, ] def train_step(self, data): with tf.GradientTape() as tape: z_mean, z_log_var, z = self.encoder(data) reconstruction = self.decoder(z) reconstruction_loss = tf.reduce_mean( tf.reduce_sum( keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2) ) ) kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)) total_loss = reconstruction_loss + kl_loss grads = tape.gradient(total_loss, self.trainable_weights) self.optimizer.apply_gradients(zip(grads, self.trainable_weights)) self.total_loss_tracker.update_state(total_loss) self.reconstruction_loss_tracker.update_state(reconstruction_loss) self.kl_loss_tracker.update_state(kl_loss) return { &quot;loss&quot;: self.total_loss_tracker.result(), &quot;reconstruction_loss&quot;: self.reconstruction_loss_tracker.result(), &quot;kl_loss&quot;: self.kl_loss_tracker.result(), } . Entrenamiento . Finalmente, instanciamos el modelo y lo compilamos. Notar que no es necesario definir una función de pérdida de manera explícita ya que la hemos definido anteriormente. Esto supone que a la hora de entrenar el modelo no hará falta proporcionar la salida espera del modelo ya que es la misma que la entrada y de ello se encarga la capa definida anteriormente. . vae = VAE(encoder, decoder) vae.compile(optimizer=keras.optimizers.Adam()) . Podemos ahora entrenar el modelo para lo que vamos a juntar los conjuntos de entrenamiento y test de MNIST. . (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() mnist_digits = np.concatenate([x_train, x_test], axis=0) mnist_digits = np.expand_dims(mnist_digits, -1).astype(&quot;float32&quot;) / 255 vae.fit(mnist_digits, epochs=30, batch_size=128) . Epoch 1/30 547/547 [==============================] - 12s 18ms/step - loss: 258.3222 - reconstruction_loss: 207.3263 - kl_loss: 3.3780 Epoch 2/30 547/547 [==============================] - 10s 18ms/step - loss: 175.8255 - reconstruction_loss: 167.4281 - kl_loss: 5.5007 Epoch 3/30 547/547 [==============================] - 10s 18ms/step - loss: 167.3953 - reconstruction_loss: 160.0686 - kl_loss: 5.8272 Epoch 4/30 547/547 [==============================] - 10s 18ms/step - loss: 162.9637 - reconstruction_loss: 155.8265 - kl_loss: 6.1042 Epoch 5/30 547/547 [==============================] - 10s 18ms/step - loss: 159.7077 - reconstruction_loss: 153.3320 - kl_loss: 6.2538 Epoch 6/30 547/547 [==============================] - 10s 18ms/step - loss: 158.1543 - reconstruction_loss: 151.6510 - kl_loss: 6.3135 Epoch 7/30 547/547 [==============================] - 10s 18ms/step - loss: 156.9544 - reconstruction_loss: 150.2694 - kl_loss: 6.3696 Epoch 8/30 547/547 [==============================] - 10s 18ms/step - loss: 155.9744 - reconstruction_loss: 149.1816 - kl_loss: 6.4029 Epoch 9/30 547/547 [==============================] - 10s 18ms/step - loss: 154.6804 - reconstruction_loss: 148.4061 - kl_loss: 6.4388 Epoch 10/30 547/547 [==============================] - 10s 18ms/step - loss: 154.5413 - reconstruction_loss: 147.5961 - kl_loss: 6.4629 Epoch 11/30 547/547 [==============================] - 10s 18ms/step - loss: 153.7180 - reconstruction_loss: 147.1163 - kl_loss: 6.4786 Epoch 12/30 547/547 [==============================] - 10s 18ms/step - loss: 153.2216 - reconstruction_loss: 146.5846 - kl_loss: 6.4871 Epoch 13/30 547/547 [==============================] - 10s 18ms/step - loss: 152.5861 - reconstruction_loss: 146.1002 - kl_loss: 6.4937 Epoch 14/30 547/547 [==============================] - 10s 18ms/step - loss: 152.2902 - reconstruction_loss: 145.7991 - kl_loss: 6.5250 Epoch 15/30 547/547 [==============================] - 10s 18ms/step - loss: 151.8982 - reconstruction_loss: 145.4320 - kl_loss: 6.5198 Epoch 16/30 547/547 [==============================] - 10s 18ms/step - loss: 151.8144 - reconstruction_loss: 145.0116 - kl_loss: 6.5122 Epoch 17/30 547/547 [==============================] - 10s 18ms/step - loss: 151.3430 - reconstruction_loss: 144.8974 - kl_loss: 6.5174 Epoch 18/30 547/547 [==============================] - 10s 18ms/step - loss: 151.0552 - reconstruction_loss: 144.5603 - kl_loss: 6.5250 Epoch 19/30 547/547 [==============================] - 10s 18ms/step - loss: 150.9264 - reconstruction_loss: 144.3094 - kl_loss: 6.5371 Epoch 20/30 547/547 [==============================] - 10s 18ms/step - loss: 149.9879 - reconstruction_loss: 143.9915 - kl_loss: 6.5339 Epoch 21/30 547/547 [==============================] - 10s 18ms/step - loss: 150.5420 - reconstruction_loss: 143.8894 - kl_loss: 6.5456 Epoch 22/30 547/547 [==============================] - 10s 18ms/step - loss: 149.8986 - reconstruction_loss: 143.6472 - kl_loss: 6.5409 Epoch 23/30 547/547 [==============================] - 10s 18ms/step - loss: 149.7602 - reconstruction_loss: 143.5481 - kl_loss: 6.5314 Epoch 24/30 547/547 [==============================] - 10s 18ms/step - loss: 150.0828 - reconstruction_loss: 143.3013 - kl_loss: 6.5392 Epoch 25/30 547/547 [==============================] - 10s 18ms/step - loss: 149.4021 - reconstruction_loss: 143.1110 - kl_loss: 6.5467 Epoch 26/30 547/547 [==============================] - 10s 18ms/step - loss: 149.6587 - reconstruction_loss: 142.8784 - kl_loss: 6.5507 Epoch 27/30 547/547 [==============================] - 10s 18ms/step - loss: 149.4968 - reconstruction_loss: 142.8175 - kl_loss: 6.5572 Epoch 28/30 547/547 [==============================] - 10s 18ms/step - loss: 148.9915 - reconstruction_loss: 142.6800 - kl_loss: 6.5604 Epoch 29/30 547/547 [==============================] - 10s 18ms/step - loss: 149.2337 - reconstruction_loss: 142.4957 - kl_loss: 6.5624 Epoch 30/30 547/547 [==============================] - 10s 18ms/step - loss: 149.0440 - reconstruction_loss: 142.3415 - kl_loss: 6.5585 . &lt;keras.callbacks.History at 0x7f09ae6fe910&gt; . Una vez que el modelo se ha entrenado, podemos usar el decoder para convertir puntos aleatorios del espacio subyacente en imágenes. . import matplotlib.pyplot as plt def plot_latent_space(vae, n=30, figsize=15): # display a n*n 2D manifold of digits digit_size = 28 scale = 1.0 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-scale, scale, n) grid_y = np.linspace(-scale, scale, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = vae.decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[ i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size, ] = digit plt.figure(figsize=(figsize, figsize)) start_range = digit_size // 2 end_range = n * digit_size + start_range pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(&quot;z[0]&quot;) plt.ylabel(&quot;z[1]&quot;) plt.imshow(figure, cmap=&quot;Greys_r&quot;) plt.show() plot_latent_space(vae) . La cuadrícula anterior muestra una distribución continua de los dígitos de las distintas clases, y se puede ver cómo un dígito se transforma en otro al seguir un camino a través del espacio subyacente. Notar que hay direcciones subyacentes que tienen significado (como cuatri-ficar o uni-ficar). . Por último podemos ver los clústeres del espacio subyacente asociados a cada clase. . def plot_label_clusters(vae, data, labels): # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(data) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels) plt.colorbar() plt.xlabel(&quot;z[0]&quot;) plt.ylabel(&quot;z[1]&quot;) plt.set_cmap(&#39;viridis&#39;) plt.show() (x_train, y_train), _ = keras.datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1).astype(&quot;float32&quot;) / 255 plot_label_clusters(vae, x_train, y_train) .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/practica/2022/02/28/practica4-autoencoder.html",
            "relUrl": "/practica/2022/02/28/practica4-autoencoder.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Segmentación semántica",
            "content": "En este notebook se muestra cómo crear un modelo de segmentación semántica usando la arquitectura U-net incluida en la librería FastAI. . En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes). . Librer&#237;as . Comenzamos actualizando la librería FastAI. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -&gt; Reiniciar Entorno de ejecución). . !pip install fastai -Uq . Cargamos a continuación las librerías que necesitaremos en esta práctica. . from fastai.basics import * from fastai.vision import models from fastai.vision.all import * from fastai.metrics import * from fastai.data.all import * from fastai.callback import * from pathlib import Path import random . Dataset . Para esta práctica vamos a usar como dataset el proporcionado en el trabajo Deep neural networks for grape bunch segmentation in natural images from a consumer‑grade camera. Este dataset dedicado a la segmentación de racimos de uva consta de 66 imágenes de entrenamiento y 14 de test con 5 categorías: background, leaves, wood, pole, y grape. Los siguientes comandos descargan y descomprimen dicho dataset. En este notebook vamos a usar solo dos clases: background y grape. . %%capture !wget https://www.dropbox.com/s/uknzc914w311web/dataset.zip?dl=1 -O dataset.zip !unzip dataset.zip . Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. . path=Path(&#39;dataset/&#39;) . Como en la práctica anterior, podemos ver el contenido de este directorio usando el comando ls(). . path.ls() . Si exploráis el directorio podréis ver que hay dos carpetas llamadas Images y Labels. La carpeta Images contiene las imágenes del dataset, y la carpeta Labels contiene las en forma de máscara. Para cada imagen, hay un fichero de anotación siguiendo la siguiente nomenclatura: si la imagen se llama color_xxx.jpg, su fichero de anotación es gt_xxx.png. El dataset está partido en entrenamiento y test como puede verse en las carpetas Images y Labels. Además, se proporcionan dos ficheros txt que van a contener las clases de los objetos que utilizaremos en esta práctica. El fichero codes.txt contiene solo dos clases (background y grape), mientras que el fichero codesAll.txt contiene todas las posibles clases. . (path/&#39;Images&#39;).ls() . (path/&#39;Images/train&#39;).ls() . (path/&#39;Labels/train&#39;).ls() . Definiciones previas . El proceso para entrenar nuestro modelo va a ser similar al visto en la práctica 1 para crear un modelo de clasificación. Sin embargo, para cargar nuestro dataset será necesario dar unas definiciones previas. Estas definiciones son necesarias para ajustar la carga del datos a la estructura de nuestro dataset. . En primer lugar vamos a definir los paths donde se van a encontrar nuestras imágenes y sus etiquetas. . path_images = path/&quot;Images&quot; path_labels = path/&quot;Labels&quot; . A continuación definimos el nombre que va a tener nuestra carpeta de test. . test_name = &quot;test&quot; . Seguidamente definimos una función que dado el path de una imagen nos devuelve el path de su anotación. . def get_y_fn (x): return Path(str(x).replace(&quot;Images&quot;,&quot;Labels&quot;).replace(&quot;color&quot;,&quot;gt&quot;).replace(&quot;.jpg&quot;,&quot;.png&quot;)) . Seguidamente cargamos las clases que pueden tener los píxeles de nuestra imágenes y lo almacenamos en una lista codes. . codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) . codes . Podemos ahora ver alguna de las imágenes de nuestro dataset. . img_f = path_images/&#39;train/color_206.jpg&#39; img = PILImage.create(img_f) img.show(figsize=(5, 5)) . Y también la anotación asociada. . mask = PILMask.create(get_y_fn(img_f)) mask.show(figsize=(5, 5), alpha=1) . Como podemos ver en la imagen anterior tenemos una máscara donde cada tipo de objeto de nuestra imagen tiene un color distinto. . Partici&#243;n del dataset . Como en cualquier problema de machine learning debemos partir nuestro dataset en entrenamiento y test. En nuestro caso los datos ya están separados por lo que vamos a definir una función que nos permite diferenciarlos gracias a la estructura de carpetas que usamos. . def ParentSplitter(x): return Path(x).parent.name==test_name . Data augmentation . Al igual que con los modelos definidos en prácticas anteriores podemos usar técnicas de aumento de datos, para lo que usaremos la librería Albumentations. Recordar que dichas transformaciones no deben aplicarse solo a la imagen sino también a su anotación. Para ello vamos a definir una clase que hereda de la clase ItemTransform y que nos va a permitir realizar transformaciones sobre pares (imagen,máscara). . La clase ItemTransform tiene un método encodes que es el encargado de realizar la transformación sobre su entrada x que en este caso será un par (imagen,máscara). Además el constructor de la clase que vamos a definir recibirá como parámetro las transformaciones a aplicar. . from albumentations import ( Compose, OneOf, ElasticTransform, GridDistortion, OpticalDistortion, HorizontalFlip, Rotate, Transpose, CLAHE, ShiftScaleRotate ) class SegmentationAlbumentationsTransform(ItemTransform): split_idx = 0 def __init__(self, aug): self.aug = aug def encodes(self, x): img,mask = x aug = self.aug(image=np.array(img), mask=np.array(mask)) return PILImage.create(aug[&quot;image&quot;]), PILMask.create(aug[&quot;mask&quot;]) . En nuestro caso vamos a utilizar solo flips horizontales, rotaciones, y una operación que aplica una pequeña distorsión a la imagen. Dichas transformaciones se aplicarán de manera secuencia y de manera aleatoria. . transforms=Compose([HorizontalFlip(p=0.5), Rotate(p=0.40,limit=10),GridDistortion() ],p=1) . Por último construimos un objeto de la clase definida anteriormente. . transformPipeline=SegmentationAlbumentationsTransform(transforms) . También va a ser necesario realizar una transformación adicional sobre las máscaras. Las máscaras contienen píxeles con 7 valores distintos (255: grape, 150: leaves, 76: pole, 74: pole, 29: wood, 25: wood, 0: background). Como vamos a trabajar únicamente con las clases grape y background, los píxeles del resto de clases deberán estar a 0 (es decir los vamos a considerar como background). Además, los números de las clases deben ser 0,1,2,... Es por esto que es necesario cambiar todos los píxeles con valor 255 a valor 1. Para realizar estas transformaciones definimos la siguiente clase. . class TargetMaskConvertTransform(ItemTransform): def __init__(self): pass def encodes(self, x): img,mask = x #Convert to array mask = np.array(mask) mask[mask!=255]=0 # Change 255 for 1 mask[mask==255]=1 # Back to PILMask mask = PILMask.create(mask) return img, mask . Dataloader . Ya estamos listos para definir nuestro DataBlock y seguidamente nuestro DataLoader. Nuestro DataBlock se define del siguiente modo. . trainDB = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), get_items=partial(get_image_files,folders=[&#39;train&#39;]), get_y=get_y_fn, splitter=RandomSplitter(valid_pct=0.2), item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline], batch_tfms=Normalize.from_stats(*imagenet_stats) ) . Vamos a explicar cada una de las componentes anteriores: . blocks=(ImageBlock, MaskBlock(codes)). En este caso tenemos que la entrada de nuestro modelo va a ser una imagen (representada mediante un ImageBlock) y su salida es una máscara (representado mediante MaskBlock) cuyos posibles valores son aquellos proporcionados por la lista de clases almacenada en la variable codes. | get_items=partial(get_image_files,folders=[&#39;train&#39;]). El parámetro get_items sirve para indicar cómo cargar los datos de nuestro dataset. Para esto vamos a usar la función get_image_files que devuelve los paths de las imágenes que se encuentran dentro de la carpeta folders (en nuestro caso la carpeta train). | get_y=get_y_fn. El parámetro get_y sirve para indicar cómo obtener la anotación asociada con una entrada (recordar que una entrada va a ser una imagen definida a partir de su path). Para esto tenemos la función get_y_fn definida anteriormente. | splitter=RandomSplitter(valid_pct=0.2). Como siempre debemos partir nuestro dataset para tener un conjunto de validación de cara a seleccionar nuestros hiperparámetros. En este caso partimos el conjunto de entrenamiento usando un porcentaje 80/20. | item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline]. En el parámetro item_tfms indicamos las transformaciones que vamos a aplicar a nuestras imágenes y sus correspondientes máscaras. Además de las explicadas anteriormente vamos a reescalar las imágenes al tamaño 480x640. | batch_tfms=Normalize.from_stats(*imagenet_stats). En el parámetro batch_tfms indicamos las transformaciones que se realizan a nivel de batch. En este caso como en nuestro modelo utilizaremos un backbone preentrenado en ImageNet debemos normalizar las imágenes para que tengan la escala de esas imágenes. | . Con las explicaciones anteriores en sencillo comprender como definimos el siguiente DataBlock que nos servirá para evaluar nuestros modelos en el conjunto de test. . testDB = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), get_items=partial(get_image_files,folders=[&#39;train&#39;,&#39;test&#39;]), get_y=get_y_fn, splitter=FuncSplitter(ParentSplitter), item_tfms=[Resize((480,640)), TargetMaskConvertTransform(), transformPipeline], batch_tfms=Normalize.from_stats(*imagenet_stats) ) . Ahora ya podemos definir nuestros Dataloaders indicando el path donde se encuentran las imágenes y el batch size que vamos a utilizar. . bs = 4 trainDLS = trainDB.dataloaders(path_images,bs=bs) testDLS = testDB.dataloaders(path_images,bs=bs) . Como siempre es conveniente mostrar un batch para comprobar que se están cargando los datos correctamente. . trainDLS.show_batch(vmin=0,vmax=1,figsize=(12, 9)) . Definici&#243;n de modelo . Ya podemos definir nuestro modelo y entrenarlo como hemos hecho en prácticas anteriores. Para ello vamos a crear un Learner mediante la función unet_learner a la cual le tenemos que proporcionar el DataLoader el backbone que vamos a utilizar (en este caso usaremos un modelo Resnet-18) y las métricas Dice y Jaccard que emplearemos para evaluar nuestro modelo. . learn = unet_learner(trainDLS,resnet18,metrics=[Dice(),JaccardCoeff()]).to_fp16() . Por último entrenamos nuestro modelo. . learn.fit_one_cycle(20,3e-3) . Una vez entrenado el modelo lo vamos a guardar para usarlo posteriormente. Lo primero que hacemos es extraer el modelo del Learner y caragarlo en la CPU. . aux=learn.model aux=aux.cpu() . Ahora vamos a guardarlo, para lo cual es necesario cargar una imagen que le servirá como referencia para realizar las transformaciones necesarias. Para ello es necesario normalizar la imagen para que sigan el estándar de ImageNet. . import torchvision.transforms as transforms img = PILImage.create(path_images/&#39;train/color_206.jpg&#39;) transformer=transforms.Compose([transforms.Resize((480,640)), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) img=transformer(img).unsqueeze(0) img=img.cpu() traced_cell=torch.jit.trace(aux, (img)) traced_cell.save(&quot;unet.pth&quot;) . Evaluando el modelo . Al igual que vimos para los modelos de clasificación, la métrica mostrada durante el proceso de entrenamiento se refiere al conjunto de validación, mientras que nos interesa saber el resultado obtenido para el conjunto de test. . Para ello debemos modificar el dataloader del objeto Learn que hemos entrenado anteriormente. . learn.dls = testDLS . Por último evaluamos nuestro modelo usando el método validate(). En este caso el método validate() devuelve tres valores, el valor de la pérdida, y el valor de las métricas definidas anteriormente con respecto al conjunto de test. . learn.validate() . Inferencia . Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar el modelo. El modelo inicialmente lo cargaremos en la CPU, pero posteriormente si hay una GPU disponible la usaremos para inferencia. . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = torch.jit.load(&quot;unet.pth&quot;) model = model.cpu() model.eval() . El siguiente paso es cargar la imagen, para lo que usaremos la librería PIL. . import PIL . img = PIL.Image.open(&#39;dataset/Images/test/color_154.jpg&#39;) . La siguiente instrucción permite mostrar la imagen que acabamos de cargar. . img . Ya estaríamos listos para relizar las predicciones sobre la imagen. Sin embargo, cabe recordar que primero debemos reescalar las imágenes y normalizarlas. . import torchvision.transforms as transforms def transform_image(image): my_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image_aux = image return my_transforms(image_aux).unsqueeze(0).to(device) . El siguiente paso consiste en transformar la imagen. . image = transforms.Resize((480,640))(img) tensor = transform_image(image=image) . Ahora ya podemos realizar pasarle el objeto construido anteriormente al modelo para realizar la predicción. . model.to(device) with torch.no_grad(): outputs = model(tensor) outputs = torch.argmax(outputs,1) . Ahora almacenamos el resultado en un array y convertimos el índice asociado con la clase grape (que era 1) al valor 255. . mask = np.array(outputs.cpu()) mask[mask==1]=255 . La predicción devuelta por el modelo es un vector de tamaño 480x640 por lo que tendremos que ponerla en forma de matriz. . mask=np.reshape(mask,(480,640)) . Con esto ya podemos mostrar la máscara generada. . Image.fromarray(mask.astype(&#39;uint8&#39;)) . Podemos compararla con la máscara real. . PIL.Image.open(&#39;dataset/Labels/test/gt_154.png&#39;) . Como vemos el modelo se aproxima bastante, pero la segmentación no es excesivamente buena. En la práctica veremos cómo crear mejores modelos. .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/practica/2022/02/22/practica3-segmentacion-semantica.html",
            "relUrl": "/practica/2022/02/22/practica3-segmentacion-semantica.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Detector de objetos",
            "content": "En este notebook se muestra cómo crear un modelo de detección de objetos usando la arquitectura Faster R-CNN. Para crear nuestro modelo vamos a utilizar la librería IceVision que es una librería para crear modelos de detección usando FastAI. . En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes). . Librer&#237;as . Comenzamos descargando la librería IceVision. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -&gt; Reiniciar Entorno de ejecución). . !pip install icevision[all] -Uq . A continuación, cargamos aquellas librerías que son necesarias. . from icevision.all import * . Dataset . Para esta práctica vamos a usar como ejemplo el Fruit Images for Object Detection dataset. Este dataset consta de 240 imágenes de entrenamiento y 60 de test con tres categorías: manzanas, plátanos y naranjas. Los siguientes comandos descargan y descomprimen dicho dataset. . %%capture !wget https://www.dropbox.com/s/1dsfd5rrmg3riqj/fruits.zip?dl=1 -O fruits.zip !unzip fruits.zip . Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. . path=Path(&#39;fruits&#39;) . Como en la práctica anterior, podemos ver el contenido de este directorio usando el comando ls(). . path.ls() . Si exploráis el directorio podréis ver que hay dos carpetas (llamadas train y test), y que cada una de ellas contiene dos carpetas, llamadas images y labels. La carpeta images contiene las imágenes del dataset, y la carpeta labels contiene las anotaciones en formato Pascal VOC. Para cada imagen, hay un fichero xml con el mismo nombre que contiene su extensión. . Icevision . El proceso para crear y evaluar un modelo de IceVision consta de los siguientes pasos: . Crear un parser para leer las imágenes y las anotaciones. | Construir objetos record a partir de los parser. | Crear los datasets a partir de los records y los aumentos que queramos aplicar. | Crear un dataloader a partir de los datasets. | Definir un modelo. | Entrenar el modelo. | Guardar el modelo. | Usar el modelo para inferencia | Vamos a ver en detalle cada uno de estos pasos. . Parser . IceVision proporciona una serie de parsers definidos por defecto para leer las anotaciones en distintos formatos entre ellos Pascal VOC y COCO. También es posible crear parsers propios. A pesar de que existe un parser para el formato de nuestra anotación, vamos a ver cómo crear un parser desde cero. . El primer paso es crear una plantilla para nuestro dataset. . template_record = ObjectDetectionRecord() . A continuación IceVision proporciona el método generate_template que nos proporciona los métodos que debemos implementar. . Parser.generate_template(template_record) . A continuación vamos a implementar nuestra clase con cada uno de esos métodos. . import xml.etree.ElementTree as ET class MyParser(Parser): &quot;&quot;&quot;Definimos el constructor de nuestra clase que va a recibir cuatro parámetros: - La plantilla definida previamente. - El path al directorio donde se encuentran las imágenes. - El path al directorio donde se encuentran las anotaciones. - Un objeto class_map con las clases que tiene nuestro dataset. &quot;&quot;&quot; def __init__(self, template_record,path_img,path_anotaciones,class_map): super().__init__(template_record=template_record) self.path_img = path_img self.path_anotaciones= path_anotaciones self.class_map = class_map &quot;&quot;&quot;El método iter escanea el directorio de anotaciones y nos devuelve el nombre de cada fichero. Dicho nombre será utilizado por el resto de método&quot;&quot;&quot; def __iter__(self): with os.scandir(self.path_anotaciones) as ficheros: for fichero in ficheros: yield fichero.name &quot;&quot;&quot;El método len nos indica el número de elementos de los que consta nuestro dataset&quot;&quot;&quot; def __len__(self): return len(self.path_anotaciones) &quot;&quot;&quot;A partir del nombre del fichero de anotación, record_id debe devolver el identificador (o nombre) de la imagen asociada&quot;&quot;&quot; def record_id(self, o) -&gt; Hashable: #o --&gt; nombre de la anotación return o[:o.find(&#39;.&#39;)] &quot;&quot;&quot;A continuación deberíamos definir el método parse_fields, pero vamos a definir una serie de definiciones previas que nos serán útiles&quot;&quot;&quot; &quot;&quot;&quot;El método prepare recibe el nombre de un fichero de anotación como parámetro y realiza una serie de labores de preprocesamiento sobre dicho fichero de anotación. En este caso lo procesa usando la funcionalidad de la librería para trabajar con xml&quot;&quot;&quot; def prepare(self, o): tree = ET.parse(str(self.path_anotaciones)+&#39;/&#39;+str(o)) self._root = tree.getroot() &quot;&quot;&quot;El método filepath a partir del nombre del fichero de anotación devuelve el path de la imagen asociada&quot;&quot;&quot; def filepath(self, o) -&gt; Union[str, Path]: path=Path(f&quot;{o[:o.find(&#39;.&#39;)]}.jpg&quot;) return self.path_img / path &quot;&quot;&quot;La función image_width_height devuelve el ancho y el alto de una imagen a partir del nombre del fichero de anotación&quot;&quot;&quot; def image_width_height(self, o) -&gt; Tuple[int, int]: return get_img_size(str(self.path_img)+&#39;/&#39;+f&quot;{o[:o.find(&#39;.&#39;)]}.jpg&quot;) &quot;&quot;&quot;La función labels recibe el nombre del fichero de anotación y debe devolver una lista con los identificadores de las clases contenidas en dicho fichero.&quot;&quot;&quot; def labels(self, o) -&gt; List[int]: labels = [] for object in self._root.iter(&quot;object&quot;): label = object.find(&quot;name&quot;).text label_id = self.class_map.get_by_name(label) labels.append(label) return labels &quot;&quot;&quot;La función bboxes recibe el nombre del fichero de anotación y debe devolver una lista de bboxes que son las anotaciones contenidas en dicho fichero. El formato de cada BBOX es xmin, ymin, xmax, ymax.&quot;&quot;&quot; def bboxes(self, o) -&gt; List[BBox]: def to_int(x): return int(float(x)) bboxes = [] for object in self._root.iter(&quot;object&quot;): xml_bbox = object.find(&quot;bndbox&quot;) xmin = to_int(xml_bbox.find(&quot;xmin&quot;).text) ymin = to_int(xml_bbox.find(&quot;ymin&quot;).text) xmax = to_int(xml_bbox.find(&quot;xmax&quot;).text) ymax = to_int(xml_bbox.find(&quot;ymax&quot;).text) bbox = BBox.from_xyxy(xmin, ymin, xmax, ymax) bboxes.append(bbox) return bboxes &quot;&quot;&quot;Definimos a continuación el método parse_fields para cada elemento de nuestro dataset proporcionamos: - El path a la imagen. - El tamaño de la imagen. - El mapa de clases. - Los rectángulos que indican cada uno de los objetos de la imagen. - Las etiquetas de cada uno de los objetos de la imagen.&quot;&quot;&quot; def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): self.prepare(o) if is_new: record.set_filepath(self.filepath(o)) record.set_img_size(self.image_width_height(o)) record.detection.set_class_map(self.class_map) record.detection.add_bboxes(self.bboxes(o)) record.detection.add_labels(self.labels(o)) . Una vez que hemos definido nuestra clase para parsear las anotaciones de nuestro dataset, vamos a construir los objetos correspondientes. . Lo primero que tenemos que hacer es construir nuestro class_map que es un objeto de la clase ClassMap y que contiene las clases de objetos de nuestro dataset. . class_map = ClassMap([&#39;apple&#39;,&#39;banana&#39;,&#39;orange&#39;]) . A continuación definimos nuestros parsers. Uno para leer el conjunto de entrenamiento, y otro para el de test. . trainPath = Path(&#39;fruits&#39;)/&#39;train&#39; parserTrain = MyParser(template_record, trainPath/&#39;images&#39;, trainPath/&#39;labels&#39;, class_map) testPath = Path(&#39;fruits&#39;)/&#39;test&#39; parserTest = MyParser(template_record,testPath/&#39;images&#39;, testPath/&#39;labels&#39;, class_map) . Records . Un record es un diccionario que contiene todos los campos parseados definidos en el proceso anterior. Mientras que cada parser es específico para cada anotación concreta, los objetos record tienen una estructura común. Para construir los records a partir de nuestros objetos parser debemos llamar al método parse e indicarle cómo se van a repartir los datos que se lean. . Como siempre, vamos a dividir nuestro dataset en tres partes: un conjunto de entrenamiento, uno de validación y uno de test. Por lo tanto tendremos que construir tres records llamados train_records, valid_records y test_records. Los records de entrenamiento y validación los construiremos a partir de los datos de entrenamiento usando una partición 90/10. Mientras que el record de test se construye a partir del conjunto de test usándolo completamente. . train_records, valid_records = parserTrain.parse(RandomSplitter((0.9, 0.1))) test_records,_= parserTest.parse(RandomSplitter((1.0, 0.0))) . Transforms . Las transformaciones o aumentos son una parte fundamental cuando estamos construyendo modelos en visión por computador. IceVision incluye por defecto la librería Albumentations que nos proporciona una gran cantidad de transformaciones. Además es capaz de gestionar los cambios en la anotación que son necesarios cuando se trabaja en detección de objetos. . IceVision proporciona una función muy útil que es tfms.A.aug_tfms con una gran cantidad de transformaciones. Además podemos añadirle cualquier otra transformación de Albumentations. . Para nuestro ejemplo vamos a usar las transformaciones sugeridas por defecto por IceVision y aplicar la técnica de presizing vista para clasificación de imágenes; además será necesario normalizar las imágenes al rango de las imágenes de ImageNet. Notar que las transformaciones se aplicarán solo al conjunto de entrenamiento. Para los conjuntos de validación y test únicamente tendremos que escalar la imagen al tamaño adecuado y normalizarla. . presize = 512 size = 384 . train_tfms = tfms.A.Adapter( [*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()] ) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) . Dataset . La clase Dataset sirve para combinar los records y las transformaciones. Debemos crear un dataset para nuestro conjunto de entrenamiento, otro para el conjunto de validación y otro para el de test. . train_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) test_ds = Dataset(test_records, valid_tfms) . Una vez creados dichos datasets podemos mostrar imágenes de los mismos. En concreto la siguiente instrucción muestra imágenes del conjunto de entrenamiento a las cuáles se les han aplicado una serie de transformaciones. Es conveniente ejecutar esta visualización para comprobar que todo está correcto. . samples = [train_ds[0] for _ in range(3)] show_samples(samples, ncols=3, class_map=class_map, denormalize_fn=denormalize_imagenet) . DataLoaders . Al igual que vimos para los modelos de clasificación de FastAI, el último paso es crear nuestros DataLoaders a partir de los datasets construidos anteriormente. Notar que cada modelo tiene su propio DataLoader. En este caso como vamos a crear un modelo de Faster RCNN debemos usar las siguientes instrucciones. . train_dl = models.torchvision.faster_rcnn.train_dl(train_ds, batch_size=8, num_workers=0, shuffle=True) valid_dl = models.torchvision.faster_rcnn.valid_dl(valid_ds, batch_size=8, num_workers=0, shuffle=False) test_dl = models.torchvision.faster_rcnn.valid_dl(test_ds, batch_size=8, num_workers=0, shuffle=False) . Entrenando el modelo . Para crear y entrenar nuestro modelo debemos crear un objeto Learner de FastAI. Para crear dicho objeto, lo primero que debemos hacer es construir un modelo con la arquitectura que queremos usar, en este caso Faster RCNN y con un backbone que es Resnet18. . model = models.torchvision.faster_rcnn.model(backbone=models.torchvision.faster_rcnn.backbones.resnet18_fpn(pretrained=True), num_classes=len(class_map)) . A continuación debemos proporcionar las métricas que queremos utilizar para evaluar el modelo. Por el momento la única métrica soportada por IceVision es el mAP de COCO, por lo tanto utilizaremos dicha métrica. . metrics = [COCOMetric(metric_type=COCOMetricType.bbox)] . Ya estamos listos para construir nuestro Learner. Notar que dicho objeto se construye de manera distinta dependiendo de la arquitectura que queramos utilizar. . learn = models.torchvision.faster_rcnn.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics) . Ahora podemos entrenar nuestro modelo utilizando la técnica de fine tuning que vimos en clase. . learn.fine_tune(10,freeze_epochs=2) . Una vez finalizado el entrenamiento podemos guardar nuestro modelo del siguiente modo. . torch.save(model.state_dict(),&#39;fasterRCNNFruits.pth&#39;) . Evaluando el modelo . Al igual que vimos para los modelos de clasificación, la métrica mostrada durante el proceso de entrenamiento se refiere al conjunto de validación, mientras que nos interesa saber el resultado obtenido para el conjunto de test. . Para ello, lo primero que debemos hacer es construir un nuevo dataloader del siguiente modo, indicando que el conjunto de validación es el de test. . newdl = fastai.DataLoaders(models.torchvision.faster_rcnn.fastai.convert_dataloader_to_fastai(train_dl), models.torchvision.faster_rcnn.fastai.convert_dataloader_to_fastai(test_dl)).to(&#39;cuda&#39;) . A continuación modificamos el dataloader del objeto Learn que hemos entrenado anteriormente. . learn.dls = newdl . Por último evaluamos nuestro modelo usando el método validate(). Al igual que en el caso de los modelos de clasificación el método validate() devuelve dos valores, el valor de la pérdida y el valor de la métrica asociada al conjunto de validación, que en este caso es el de test. . learn.validate() . Inferencia . Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo. Para ello debemos crear un modelo con la arquitectura que utilizamos (Faster RCNN en nuestro caso), y posteriormente cargar el modelo. . model = models.torchvision.faster_rcnn.model(backbone=models.torchvision.faster_rcnn.backbones.resnet18_fpn, num_classes=len(class_map)) state_dict = torch.load(&#39;fasterRCNNFruits.pth&#39;) model.load_state_dict(state_dict) . El siguiente paso es cargar la imagen, para lo que usaremos la librería PIL. . import PIL . img = PIL.Image.open(&#39;fruits/test/images/mixed_25.jpg&#39;) . La siguiente instrucción permite mostrar la imagen que acabamos de cargar. . img . Ya estaríamos listos para relizar las predicciones sobre la imagen. Sin embargo, cabe recordar que primero debemos reescalar las imágenes y normalizarlas. . infer_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size),tfms.A.Normalize()]) . Ya podemos realizar las predicciones mediante el método end2end_detect. Este método, que depende de la arquitectura que hayamos utilizado, recibe como parámetros la imagen sobre la que queremos realizar las predicciones, las transformaciones a aplicar, el modelo (movido a la CPU), el mapa de clases, y el nivel de confianza mínimo para realizar la predicción. . pred_dict = models.torchvision.faster_rcnn.end2end_detect(img, infer_tfms, model.to(&quot;cpu&quot;), class_map=class_map, detection_threshold=0.5) pred_dict[&#39;img&#39;] .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/practica/2022/02/20/practica2-deteccion-de-objetos.html",
            "relUrl": "/practica/2022/02/20/practica2-deteccion-de-objetos.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Clasificación de imágenes",
            "content": "En este notebook se muestra cómo crear un modelo de clasificación de imágenes utilizando las técnicas vistas en clase. . Para crear nuestro clasificador de imágenes vamos a utilizar la librería fastAI. Este notebook está inspirado en el curso asociado a dicha librería. . En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes). . Librer&#237;as . Comenzamos descargando la última versión de la librería FastAI. Al finalizar la instalación se reiniciará el kernel de manera automática. . !pip install fastai -Uq import IPython IPython.Application.instance().kernel.do_shutdown(True) . A continuación, cargamos aquellas librerías que son necesarias. . from fastai.vision.all import * . Dataset . Para esta práctica vamos a usar como ejemplo de dataset el Intel Image Classification dataset. Este dataset consta de imágenes de tamaño 150x150 distribuidas en 6 categorías (buildings, forest, glacier, mountain, sea, street). Los siguientes comandos descargan y descomprimen dicho dataset. . !wget https://unirioja-my.sharepoint.com/:u:/g/personal/joheras_unirioja_es/EbMVHqKMSnNHh6I0-4-QWdQBlVDKz2Uz5Ky73zc5tHGofg?download=1 -O IntelImageClassification.zip !unzip IntelImageClassification.zip . Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. . path = Path(&#39;IntelImageClassification/&#39;) . Con el objeto path podemos utilizar funciones como ls(). . path.ls() . Vemos que nuestro dataset consta de dos carpetas llamadas train y test. Recordar que es importante hacer la partición del dataset en dos conjuntos distintos, para luego poder evaluarlo correctamente. Podemos ahora crear objetos path que apunten respectivamente a nuestro conjunto de entrenamiento y a nuestro conjunto de test. . trainPath = path/&#39;train&#39; testPath = path/&#39;test&#39; . Veamos el contenido de cada uno de estos directorios. . trainPath.ls() . testPath.ls() . Podemos ver que tanto la carpeta train como la carpeta test contienen 6 subcarpetas, una por cada clase del dataset. . Cargando el dataset . A continuación vamos a mostrar cómo se carga el dataset para poder posteriormente crear nuestro modelo. Este proceso se hace en dos pasos. Primero se construye un objeto DataBlock y a continuación se construye un objeto DataLoader a partir del DataBlock. Tienes más información sobre estos objetos en la documentación de FastAI. . Datablock . Comenzamos construyendo el objeto DataBlock. A continuación explicaremos cada una de sus componentes. . db = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2,seed=42), get_y=parent_label, item_tfms = Resize(256), batch_tfms=aug_transforms(size=128,min_scale=0.75)) . Vamos a ver las distintas componentes del DataBlock. . El atributo blocks sirve para indicar el tipo de nuestros datos. Como estamos en un problema de clasificación de imágenes, tenemos que la entrada de nuestro modelo será una imagen, es decir un ImageBlock, y la salida será una categoría, es decir un CategoryBlock. Por lo tanto indicamos que blocks = (ImageBlock, CategoryBlock). | El atributo get_items debe proporcionar una función para leer los datos. En nuestro caso queremos leer una serie de imágenes que estarán almacenadas en un path. Para ello usamos la función get_image_files. Puedes ver qué hace exactamente esta función ejecutando el comando ??get_image_files. | El atributo splitter nos indica cómo partir el dataset. Daros cuenta que tenemos un conjunto de entrenamiento y uno de test, pero para entrenar nuestro modelo y probar distintas alternativas nos interesa usar un conjunto de validación, que lo vamos a tomar de forma aleatorea a partir de nuestro conjunto de entrenamiento usando un 20% del mismo. Para ello usaremos el objeto RandomSplitter(valid_pct=0.2,seed=42). | El atributo get_y sirve para indicar cómo extraemos la clase a partir de nuestros datos. La función get_image_files nos proporciona una lista con los paths a las imágenes de nuestro dataset. Si nos fijamos en dichos paths, la clase de cada imagen viene dada por la carpeta en la que está contenida, por lo que podemos usar el método parent_label para obtener la clase de la misma. | . Por último, los atributos item_tfms y batch_tfms sirven para aplicar una técnica conocida como preescalado (o presizing). . Preescalado (presizing) . El preescalado es una técnica de aumento de datos diseñada para minimizar la destrucción de datos. Para poder sacar el máximo partido a las GPUs, es necesario que todas las imágenes tengan el mismo tamaño, por lo que es común reescalar todas las imágenes al mismo tamaño. . Sin embargo, hay varias técnicas de aumento que si se aplican después de reescalar pueden introducir zonas vacías o degradar los datos. Por ejemplo, si rotamos una imagen 45 grados, los bordes de la imagen quedan vacíos, lo que no le sirve para nada al modelo. Para solucionar este problema se utiliza la técnica del preescalado que consta de dos pasos. . Las imágenes se reescalan a una dimensión mayor que la que se usará realmente para entrenar. | Se aplican las distintas técnicas de aumento, y finalmente se reescala al tamaño deseado. | El punto clave es el primer paso que sirve para crear imágenes con el suficiente espacio para luego poder aplicar los distintos aumentos. Tienes más información sobre esta técnica en el libro de FastAI. . En nuestro caso estamos haciendo un escalado inicial a tamaño 256 para luego aplicar un escalado a tamaño 128. Notar que no sólo estamos aplicando un escalado como técnica de aumento de datos, sino que también gracias a la función aug_transforms se aplican otros aumentos. . Data augmentation . Como hemos visto en clase, la técnica de aumento de datos (o data augmentation) nos proporciona un método para aumentar el tamaño de nuestro dataset. FastAI ofrece una serie de aumentos por defecto que se pueden configurar mediante el método aug_transforms. Veamos a continuación que aumentos ofrece dicha función. . ??aug_transforms . Como vemos la función anterior puede ser utilizada para fijar distintos aumentos y la probabilidad con la que queremos que se apliquen. En caso de querer otro tipo de transformaciones que no estén incluidas por defecto en dicha función podemos usar la librería Albumentations como se explica en la documentación de FastAI. . Dataloader . Pasamos ahora a construir nuestro DataLoader que se construye a partir del DataBlock construido anteriormente indicándole el path donde se encuentran nuestras imágenes. Además podemos configurar el DataLoader indicándole el tamaño del batch que queremos utilizar. Al trabajar con GPUs es importante que usemos batches de tamaño 2^n para optimizar el uso de la GPU. . dls = db.dataloaders(trainPath,bs=128) . A continuación mostramos un batch de nuestro DataLoader. Es conveniente comprobar que realmente se han cargado las imágenes y sus anotaciones de manera correcta. . dls.show_batch() . Entrenando el modelo . Pasamos ahora a construir y entrenar nuestro modelo. Pero antes vamos a definir una serie de callbacks. . Callbacks . En ocasiones nos interesa cambiar el comportamiento por defecto que tiene el proceso de entrenamiento, por ejemplo para guardar los mejores pesos que se han producido hasta ese momento. El procedimiento usado por FastAI para incluir dicha funcionalidad son los callbacks que sirven para modificar el proceso de entrenamiento. La lista completa de callbacks incluida en FastAI, está disponible en su documentación. En nuesto caso sólo vamos a utilizar 3 callbacks: . ShowGraphCallback: este callback sirve para mostrar las curvas de entrenamiento y validación. | EarlyStoppingCallback: este callback nos permite aplicar la técnica de early stopping. Para ello debemos indicarle la métrica que queremos monitorizar para saber cuándo parar, y la paciencia (es decir cuántas épocas dejamos que el modelo continúe entrenando si no ha habido mejora). | SaveModelCallback: este callback guarda el mejor modelo encontrado durante el proceso de entrenamiento y lo carga al final del mismo. Como vamos a crear un modelo usando la arquitectura resnet18 conviene que indiquemos esto en el nombre del modelo. También sería conveniente indicar el nombre del dataset para no confundirlos. | . callbacks = [ ShowGraphCallback(), EarlyStoppingCallback(patience=3), SaveModelCallback(fname=&#39;modelResnet18&#39;) ] . Además de estos tres callbacks utilizaremos otro que nos servirá para acelerar el entrenamiento de nuestros modelos usando mixed precision. . Construyendo el modelo . A continuación construimos nuestro modelo, un objeto de la clase Learner, utilizando el método cnn_learner que toma como parámetros el DataLoader, la arquitectura que queremos entrenar (en nuestro caso un resnet18), la métrica que usaremos para evaluar nuestro modelo (esta evaluación se hace sobre el conjunto de validación, y en nuestro caso será la accuracy), y los callbacks. Notar que en la instrucción anterior incluimos la transformación del modelo a mixed precision. . learn = cnn_learner(dls,resnet18,metrics=accuracy,cbs=callbacks).to_fp16() . Notar que internamente la función cnn_learner hace varias cosas con la arquitectura que le pasamos como parámetro (en este caso resnet18). Dicha arquitectura fue entrenada inicialmente para el problema de ImageNet, por lo que ante una nueva imagen, su salida sería la predicción en una de las 1000 clases de ImageNet. Sin embargo, internamente la función cnn_learner elimina las últimas capas de dicha arquitectura, y las reemplaza con una adecuada para nuestro problema concreto. . Antes de entrenar nuestro modelo debemos encontrar un learning rate adecuado. . Learning rate finder . Como hemos visto en teoría, el trabajo de Leslie Smith proporciona un método para encontrar un learning rate adecuado para entrenar nuestro modelo. Dicho learning rate lo puedes encontrar con la función lr_find(). . learn.lr_find() . La función anterior no solo nos devuelve un gráfico sino que nos sugiere dos valores lr_min y lr_steep. La recomendación es utilizar el valor de lr_steep, para entrenar el modelo. . Fine-tuning . A continuación vamos a aplicar la técnica de fine tuning. En FastAI esto es tan sencillo como llamar al método fine_tune del objeto Learner. Este método recibe dos parámetros principalmente, el número de épocas (10 en nuestro caso) y el learning rate. El proceso que sigue para entrenar consiste en: . Congelar todas las capas salvo la última, y entrenar esa parte del modelo durante una época. | Descongelar la red, y entrenar el modelo por el número de épocas indicado. | Al ejecutar la siguiente instrucción aparecerá una tabla donde podrás ver la pérdida para el conjunto de entrenamiento, la pérdida para el conjunto de validación, y la accuracy para el conjunto de validación. . learn.fine_tune(10,base_lr=1e-3) . Al final del entrenamiento se ha guardado un modelo en la carpeta models que contiene el mejor modelo construido. . Path(&#39;models&#39;).ls() . Para su uso posterior, es conveniente exportar el modelo. Para ello es necesario en primer lugar convertir el modelo a fp32. . learn.to_fp32() learn.export() . Podemos ver que dicho modelo se ha guardado en el mismo directorio donde nos encontramos. . Path().ls(file_exts=&#39;.pkl&#39;) . Evaluando el modelo . Una vez tenemos entrenado nuestro modelo nos interesa saber: . ¿Qué tal funciona en el conjunto de test? | ¿Qué errores comete? | ¿Cómo se utiliza para predecir la clase ante nuevas imágenes? | Evaluando el modelo en el conjunto de test . Para poder evaluar nuestro modelo en el conjunto de test debemos crear un nuevo DataBlock y un nuevo DataLoader. La única diferencia con el DataBlock utilizado previamente es que para hacer la partición del dataset usamos un objeto de la clase GrandparentSplitter indicando que el conjunto de validación es nuestro conjunto de test. En el caso del DataLoader, la diferencia con el definido anteriormente es que cambiamos la ruta al path. . dbTest = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;test&#39;), get_y=parent_label, item_tfms = Resize(256), batch_tfms=aug_transforms(size=128,min_scale=0.75)) dlsTest = dbTest.dataloaders(path,bs=128) . Para trabajar con este dataloader debemos modificar nuestro objeto Learner. En concreto su atributo dls. . learn.dls = dlsTest . Ahora podemos evaluar nuestro modelo usando el método validate. . learn.validate() . El método validate nos devuelve dos valores: el valor de la función de pérdida, y el valor de nuestra métrica (la accuracy en este caso). Por lo que podemos ver que el modelo tiene una accuracy en el conjunto de test de aproximadamente un 82% (esto puede variar dependiendo de la ejecución). . Interpretaci&#243;n del modelo . Hemos visto que nuestro modelo obtiene una accuracy aproximada (puede variar debido a la aleatoriedad del proceso de entrenamiento) de un 92% en el conjunto de validación. Pero nos interesa conocer los errores que se cometen y si estos son razonables. Para ello podemos construir un objeto ClassificationInterpretation a partir de nuestro Learner y mostrar la matriz de confusión asociada. Recordar que hemos cambiado el DataLoader en el paso anterior, porque es conveniente volver al dataloader usado inicialmente. . learn.dls=dls . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12),dpi=60) . Si nos fijamos, la mayoría de errores se producen porque el modelo confunde imágenes de la clase street con imágenes de la clase buildings; e imágenes de la clase mountain con la clase glacier. Podemos ver aquellas en las que se produce un mayor error del siguiente modo. Normalmente utilizaríamos el comando interp.plot_top_losses(10,nrows=2), pero como se muestra en el foro de FastAI, esto produce un comportamiento anómalo que será corregido en próximas versiones, por lo que tenemos que usar la solución que proporcionan en dicho hilo que consiste en definir la siguiente función. . def plot_top_losses_fix(interp, k, largest=True, **kwargs): losses,idx = interp.top_losses(k, largest) if not isinstance(interp.inputs, tuple): interp.inputs = (interp.inputs,) if isinstance(interp.inputs[0], Tensor): inps = tuple(o[idx] for o in interp.inputs) else: inps = interp.dl.create_batch(interp.dl.before_batch([tuple(o[i] for o in interp.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (interp.targs if is_listy(interp.targs) else (interp.targs,))) x,y,its = interp.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (interp.decoded if is_listy(interp.decoded) else (interp.decoded,))) x1,y1,outs = interp.dl._pre_show_batch(b_out, max_n=k) if its is not None: #plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), L(self.preds).itemgot(idx), losses, **kwargs) plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), interp.preds[idx], losses, **kwargs) #TODO: figure out if this is needed #its None means that a batch knows how to show itself as a whole, so we pass x, x1 #else: show_results(x, x1, its, ctxs=ctxs, max_n=max_n, **kwargs) . Una vez definida dicha función ya podemos ver los mayores errores. . plot_top_losses_fix(interp,10,nrows=2) . A partir de la ejecución anterior, podemos ver que hay algunas imágenes que están mal anotadas, y otras en las que es comprensible por qué se está produciendo el error. . Usando el modelo . Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo. . learn_inf = load_learner(&#39;export.pkl&#39;) . Ahora podemos usar dicho modelo para hacer inferencia con una nueva imagen mediante el método predict. En nuestro caso vamos a usar una imagen del conjunto de test. . learn_inf.predict(&#39;IntelImageClassification/test/buildings/20057.jpg&#39;) . La función anterior devuelve tres valores: . La clase (buildings en este caso). | El índice asociado a dicha clase. | Las probabilidades para cada una de las categorías. | . Creando una aplicaci&#243;n para nuestro modelo . Es fundamental que los modelos sean usables, por lo que es conveniente proporcionar una interfaz secilla que permita usar nuestros modelos. Para ello, vamos a usar dos herramientas: Gradio y los espacios de HuggingFace. En concreto vamos a ver cómo construir la siguiente aplicación siguiendo los pasos del blog de Tanishq Abraham. . Para crear nuestra aplicación vamos a seguir los siguientes pasos: . Descarga el fichero export.pkl creado anteriormente. | Descarga dos de las imágenes del dataset. | Crea una cuenta en HuggingFace. Este paso sólo hay que realizarlo una vez. | Crea un espacio en HuggingFace. Al crear dicho espacio usamos como nombre Practica1 y seleccionamos Gradio como SDK. | Una vez creado el espacio vamos a la pestaña Files and versions. En dicha pestaña debemos: Subir el fichero export.pkl mediante el botón Add file -&gt; Upload file. | Sube las dos imágenes que hayas descargado mediante el botón Add file -&gt; Upload file. | Crear un fichero requirements.txt mediante el botón Add file -&gt; Create a new file. Este fichero contendrá las librerías que son necesarias instalar para ejecutar nuestra aplicación, en este caso fastai. | Crear un fichero app.py mediante el botón Add file -&gt; Create a new file. Este fichero contendrá el código de la aplicación. | . | Siguiendo estos pasos tendrás una aplicación que podrás ver desde la pestaña App (la construcción de la aplicación puede llevar unos segundos, el proceso de construcción lo podrás ver pulsando en el botón See logs. |",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/practica/2022/01/10/practica1-clasificacion-imagenes.html",
            "relUrl": "/practica/2022/01/10/practica1-clasificacion-imagenes.html",
            "date": " • Jan 10, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Página creada por Jónathan Heras. .",
          "url": "https://ap2122.github.io/aprendizajeprofundo2122/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ap2122.github.io/aprendizajeprofundo2122/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}