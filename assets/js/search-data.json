{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "En este notebook se muestra cómo crear un modelo de clasificación de imágenes utilizando las técnicas vistas en clase. A lo largo del notebook aparecen una serie de preguntas que debes responder. . Para crear nuestro clasificador de imágenes vamos a utilizar la librería fastAI. Este notebook está inspirado en el curso asociado a dicha librería. . En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes). . Librer&#237;as . Comenzamos descargando la última versión de la librería FastAI. Al finalizar la instalación se reiniciará el kernel de manera automática. . !pip install fastai --upgrade import IPython IPython.Application.instance().kernel.do_shutdown(True) . A continuación, cargamos aquellas librerías que son necesarias. . from fastai.vision.all import * . Dataset . Para esta práctica vamos a usar como ejemplo de dataset el Intel Image Classification dataset. Este dataset consta de imágenes de tamaño 150x150 distribuidas en 6 categorías (buildings, forest, glacier, mountain, sea, street). Los siguientes comandos descargan y descomprimen dicho dataset. . !wget https://unirioja-my.sharepoint.com/:u:/g/personal/joheras_unirioja_es/EbMVHqKMSnNHh6I0-4-QWdQBlVDKz2Uz5Ky73zc5tHGofg?download=1 -O IntelImageClassification.zip !unzip IntelImageClassification.zip . Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. . path = Path(&#39;IntelImageClassification/&#39;) . Con el objeto path podemos utilizar funciones como ls(). . path.ls() . Vemos que nuestro dataset consta de dos carpetas llamadas train y test. Recordar que es importante hacer la partición del dataset en dos conjuntos distintos, para luego poder evaluarlo correctamente. Podemos ahora crear objetos path que apunten respectivamente a nuestro conjunto de entrenamiento y a nuestro conjunto de test. . trainPath = path/&#39;train&#39; testPath = path/&#39;test&#39; . Veamos el contenido de cada uno de estos directorios. . trainPath.ls() . testPath.ls() . Podemos ver que tanto la carpeta train como la carpeta test contienen 6 subcarpetas, una por cada clase del dataset. . Ejercicio . ¿Qué contiene la carpeta buildings de la carpeta test? Ejecuta a continuación el comando necesario para comprobarlo. . Cargando el dataset . A continuación vamos a mostrar cómo se carga el dataset para poder posteriormente crear nuestro modelo. Este proceso se hace en dos pasos. Primero se construye un objeto DataBlock y a continuación se construye un objeto DataLoader a partir del DataBlock. Tienes más información sobre estos objetos en la documentación de FastAI. . Datablock . Comenzamos construyendo el objeto DataBlock. A continuación explicaremos cada una de sus componentes. . db = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2,seed=42), get_y=parent_label, item_tfms = Resize(256), batch_tfms=aug_transforms(size=128,min_scale=0.75)) . Vamos a ver las distintas componentes del DataBlock. . El atributo blocks sirve para indicar el tipo de nuestros datos. Como estamos en un problema de clasificación de imágenes, tenemos que la entrada de nuestro modelo será una imagen, es decir un ImageBlock, y la salida será una categoría, es decir un CategoryBlock. Por lo tanto indicamos que blocks = (ImageBlock, CategoryBlock). | El atributo get_items debe proporcionar una función para leer los datos. En nuestro caso queremos leer una serie de imágenes que estarán almacenadas en un path. Para ello usamos la función get_image_files. Puedes ver qué hace exactamente esta función ejecutando el comando ??get_image_files. | El atributo splitter nos indica cómo partir el dataset. Daros cuenta que tenemos un conjunto de entrenamiento y uno de test, pero para entrenar nuestro modelo y probar distintas alternativas nos interesa usar un conjunto de validación, que lo vamos a tomar de forma aleatorea a partir de nuestro conjunto de entrenamiento usando un 20% del mismo. Para ello usaremos el objeto RandomSplitter(valid_pct=0.2,seed=42). | El atributo get_y sirve para indicar cómo extraemos la clase a partir de nuestros datos. La función get_image_files nos proporciona una lista con los paths a las imágenes de nuestro dataset. Si nos fijamos en dichos paths, la clase de cada imagen viene dada por la carpeta en la que está contenida, por lo que podemos usar el método parent_label para obtener la clase de la misma. | . Por último, los atributos item_tfms y batch_tfms sirven para aplicar una técnica conocida como preescalado (o presizing). . Preescalado (presizing) . El preescalado es una técnica de aumento de datos diseñada para minimizar la destrucción de datos. Para poder sacar el máximo partido a las GPUs, es necesario que todas las imágenes tengan el mismo tamaño, por lo que es común reescalar todas las imágenes al mismo tamaño. . Sin embargo, hay varias técnicas de aumento que si se aplican después de reescalar pueden introducir zonas vacías o degradar los datos. Por ejemplo, si rotamos una imagen 45 grados, los bordes de la imagen quedan vacíos, lo que no le sirve para nada al modelo. Para solucionar este problema se utiliza la técnica del preescalado que consta de dos pasos. . Las imágenes se reescalan a una dimensión mayor que la que se usará realmente para entrenar. | Se aplican las distintas técnicas de aumento, y finalmente se reescala al tamaño deseado. | El punto clave es el primer paso que sirve para crear imágenes con el suficiente espacio para luego poder aplicar los distintos aumentos. Tienes más información sobre esta técnica en el libro de FastAI. . En nuestro caso estamos haciendo un escalado inicial a tamaño 256 para luego aplicar un escalado a tamaño 128. Notar que no sólo estamos aplicando un escalado como técnica de aumento de datos, sino que también gracias a la función aug_transforms se aplican otros aumentos. . Data augmentation . Como hemos visto en clase, la técnica de aumento de datos (o data augmentation) nos proporciona un método para aumentar el tamaño de nuestro dataset. FastAI ofrece una serie de aumentos por defecto que se pueden configurar mediante el método aug_transforms. Veamos a continuación que aumentos ofrece dicha función. . ??aug_transforms . Como vemos la función anterior puede ser utilizada para fijar distintos aumentos y la probabilidad con la que queremos que se apliquen. En caso de querer otro tipo de transformaciones que no estén incluidas por defecto en dicha función podemos usar la librería Albumentations como se explica en la documentación de FastAI. . Dataloader . Pasamos ahora a construir nuestro DataLoader que se construye a partir del DataBlock construido anteriormente indicándole el path donde se encuentran nuestras imágenes. Además podemos configurar el DataLoader indicándole el tamaño del batch que queremos utilizar. Al trabajar con GPUs es importante que usemos batches de tamaño 2^n para optimizar el uso de la GPU. . dls = db.dataloaders(trainPath,bs=128) . A continuación mostramos un batch de nuestro DataLoader. Es conveniente comprobar que realmente se han cargado las imágenes y sus anotaciones de manera correcta. . dls.show_batch() . Entrenando el modelo . Pasamos ahora a construir y entrenar nuestro modelo. Pero antes vamos a definir una serie de callbacks. . Callbacks . En ocasiones nos interesa cambiar el comportamiento por defecto que tiene el proceso de entrenamiento, por ejemplo para guardar los mejores pesos que se han producido hasta ese momento. El procedimiento usado por FastAI para incluir dicha funcionalidad son los callbacks que sirven para modificar el proceso de entrenamiento. La lista completa de callbacks incluida en FastAI, está disponible en su documentación. En nuesto caso sólo vamos a utilizar 3 callbacks: . ShowGraphCallback: este callback sirve para mostrar las curvas de entrenamiento y validación. | EarlyStoppingCallback: este callback nos permite aplicar la técnica de early stopping. Para ello debemos indicarle la métrica que queremos monitorizar para saber cuándo parar, y la paciencia (es decir cuántas épocas dejamos que el modelo continúe entrenando si no ha habido mejora). | SaveModelCallback: este callback guarda el mejor modelo encontrado durante el proceso de entrenamiento y lo carga al final del mismo. Como vamos a crear un modelo usando la arquitectura resnet18 conviene que indiquemos esto en el nombre del modelo. También sería conveniente indicar el nombre del dataset para no confundirlos. | . callbacks = [ ShowGraphCallback(), EarlyStoppingCallback(patience=3), SaveModelCallback(fname=&#39;modelResnet18&#39;) ] . Además de estos tres callbacks utilizaremos otro que nos servirá para acelerar el entrenamiento de nuestros modelos usando mixed precision. . Construyendo el modelo . A continuación construimos nuestro modelo, un objeto de la clase Learner, utilizando el método cnn_learner que toma como parámetros el DataLoader, la arquitectura que queremos entrenar (en nuestro caso un resnet18), la métrica que usaremos para evaluar nuestro modelo (esta evaluación se hace sobre el conjunto de validación, y en nuestro caso será la accuracy), y los callbacks. Notar que en la instrucción anterior incluimos la transformación del modelo a mixed precision. . learn = cnn_learner(dls,resnet18,metrics=accuracy,cbs=callbacks).to_fp16() . Notar que internamente la función cnn_learner hace varias cosas con la arquitectura que le pasamos como parámetro (en este caso resnet18). Dicha arquitectura fue entrenada inicialmente para el problema de ImageNet, por lo que ante una nueva imagen, su salida sería la predicción en una de las 1000 clases de ImageNet. Sin embargo, internamente la función cnn_learner elimina las últimas capas de dicha arquitectura, y las reemplaza con una adecuada para nuestro problema concreto. . Antes de entrenar nuestro modelo debemos encontrar un learning rate adecuado. . Learning rate finder . Como hemos visto en teoría, el trabajo de Leslie Smith proporciona un método para encontrar un learning rate adecuado para entrenar nuestro modelo. Dicho learning rate lo puedes encontrar con la función lr_find(). . learn.lr_find() . La función anterior no solo nos devuelve un gráfico sino que nos sugiere dos valores lr_min y lr_steep. La recomendación es utilizar el valor de lr_steep, para entrenar el modelo. . Fine-tuning . A continuación vamos a aplicar la técnica de fine tuning. En FastAI esto es tan sencillo como llamar al método fine_tune del objeto Learner. Este método recibe dos parámetros principalmente, el número de épocas (10 en nuestro caso) y el learning rate. El proceso que sigue para entrenar consiste en: . Congelar todas las capas salvo la última, y entrenar esa parte del modelo durante una época. | Descongelar la red, y entrenar el modelo por el número de épocas indicado. | Al ejecutar la siguiente instrucción aparecerá una tabla donde podrás ver la pérdida para el conjunto de entrenamiento, la pérdida para el conjunto de validación, y la accuracy para el conjunto de validación. . learn.fine_tune(10,base_lr=1e-3) . Al final del entrenamiento se ha guardado un modelo en la carpeta models que contiene el mejor modelo construido. . Path(&#39;models&#39;).ls() . Para su uso posterior, es conveniente exportar el modelo. . learn.export() . Podemos ver que dicho modelo se ha guardado en el mismo directorio donde nos encontramos. . Path().ls(file_exts=&#39;.pkl&#39;) . Evaluando el modelo . Una vez tenemos entrenado nuestro modelo nos interesa saber: . ¿Qué tal funciona en el conjunto de test? | ¿Qué errores comete? | ¿Cómo se utiliza para predecir la clase ante nuevas imágenes? | Evaluando el modelo en el conjunto de test . Para poder evaluar nuestro modelo en el conjunto de test debemos crear un nuevo DataBlock y un nuevo DataLoader. La única diferencia con el DataBlock utilizado previamente es que para hacer la partición del dataset usamos un objeto de la clase GrandparentSplitter indicando que el conjunto de validación es nuestro conjunto de test. En el caso del DataLoader, la diferencia con el definido anteriormente es que cambiamos la ruta al path. . dbTest = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;test&#39;), get_y=parent_label, item_tfms = Resize(256), batch_tfms=aug_transforms(size=128,min_scale=0.75)) dlsTest = dbTest.dataloaders(path,bs=128) . Para trabajar con este dataloader debemos modificar nuestro objeto Learner. En concreto su atributo dls. . learn.dls = dlsTest . Ahora podemos evaluar nuestro modelo usando el método validate. . learn.validate() . El método validate nos devuelve dos valores: el valor de la función de pérdida, y el valor de nuestra métrica (la accuracy en este caso). Por lo que podemos ver que el modelo tiene una accuracy en el conjunto de test de aproximadamente un 82% (esto puede variar dependiendo de la ejecución). . Interpretaci&#243;n del modelo . Hemos visto que nuestro modelo obtiene una accuracy aproximada (puede variar debido a la aleatoriedad del proceso de entrenamiento) de un 92% en el conjunto de validación. Pero nos interesa conocer los errores que se cometen y si estos son razonables. Para ello podemos construir un objeto ClassificationInterpretation a partir de nuestro Learner y mostrar la matriz de confusión asociada. Recordar que hemos cambiado el DataLoader en el paso anterior, porque es conveniente volver al dataloader usado inicialmente. . learn.dls=dls . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12),dpi=60) . Si nos fijamos, la mayoría de errores se producen porque el modelo confunde imágenes de la clase street con imágenes de la clase buildings; e imágenes de la clase mountain con la clase glacier. Podemos ver aquellas en las que se produce un mayor error del siguiente modo. . interp.plot_top_losses(10,nrows=2) . A partir de la ejecución anterior, podemos ver que hay algunas imágenes que están mal anotadas, y otras en las que es comprensible por qué se está produciendo el error. . Usando el modelo . Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo. . learn_inf = load_learner(&#39;export.pkl&#39;) . Ahora podemos usar dicho modelo para hacer inferencia con una nueva imagen mediante el método predict. En nuestro caso vamos a usar una imagen del conjunto de test. . learn_inf.predict(&#39;IntelImageClassification/test/buildings/20057.jpg&#39;) . La función anterior devuelve tres valores: . La clase (buildings en este caso). | El índice asociado a dicha clase. | Las probabilidades para cada una de las categorías. | .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/jupyter/2022/01/10/practica1-clasificacion-imagenes.html",
            "relUrl": "/jupyter/2022/01/10/practica1-clasificacion-imagenes.html",
            "date": " • Jan 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ap2122.github.io/aprendizajeprofundo2122/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ap2122.github.io/aprendizajeprofundo2122/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ap2122.github.io/aprendizajeprofundo2122/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}